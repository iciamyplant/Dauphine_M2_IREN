{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n",
    "Dans ce notebook, vous allez programmer l'algorithme de descente de gradient dans sa version ordinaire, stochastique et mini-batch.\n",
    "Dans un premier temps, vous utiliserez un jeu de données simple pour tester votre code dans le cas d'une régressison linéaire. Dans un second temps, vous implémenterz vos algorithmes sur un jeu de donnée plus conséquent et comparerez vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un jeu de données simple pour tester votre code\n",
    "Un je de donnée simple constitué de 100 instance est créé artificiellement à l'aide de la relation linéaire $\\ y_{i} = 2+5\\times x_{i}  $<br>\n",
    "Ce jeu de donnée ne comporte qu'une seule variables $\\ x  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un vecteur X de taille 100 \n",
    "X=np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez X ainsi que ses dimensions\n",
    "print('X =', X) #on affiche X\n",
    "print('Dimension =', X.shape) #montre que X est un tableau à une dimension 100x1\n",
    "print('Taille =', len(X)) #donne la taille de X\n",
    "print('Type =', type(X)) #affichons aussi le type de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clcul de Y\n",
    "Y=2+5*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = [  2   7  12  17  22  27  32  37  42  47  52  57  62  67  72  77  82  87\n",
      "  92  97 102 107 112 117 122 127 132 137 142 147 152 157 162 167 172 177\n",
      " 182 187 192 197 202 207 212 217 222 227 232 237 242 247 252 257 262 267\n",
      " 272 277 282 287 292 297 302 307 312 317 322 327 332 337 342 347 352 357\n",
      " 362 367 372 377 382 387 392 397 402 407 412 417 422 427 432 437 442 447\n",
      " 452 457 462 467 472 477 482 487 492 497]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de Y ainsi que ses dimensions\n",
    "print('Y =', Y)\n",
    "print('Dimension =', Y.shape)\n",
    "print('Taille =', len(Y))\n",
    "print('Type =', type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre jeu de données avec les valeurs $\\ x_{i} $ et leur label $\\ y_{i} $ correspondant, il faut transformer le vecteur $\\ X $ poour la faire correpondre à une matrice de forme générale :\n",
    "\n",
    "$$\\begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1j} & ... & x_{1n}\\\\ 1 & x_{21} & x_{22} & ... & x_{2j} & ... & x_{2n} \\\\...&...&...&...&...&...&... \\\\1 & x_{i1} & x_{i2} & ... & x_{ij} & ... & x_{in}\\\\...&...&...&...&...&...&...\\\\1 & x_{m1} & x_{m2} & ... & x_{mj} & ... & x_{mn} \\end{bmatrix}$$ <br>\n",
    "Nous réaliserons cette transformation à l'aide d'une fonction ***Stand_Trans***  qui prend en entrée la matrice $\\ X $ à transformer, le nombre de d'instances $\\ m $ et le nombre de variables $\\ n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stand_Trans(X,m,n):\n",
    "    X=X.reshape(m,n) # Transorme la matrice en entrée en une matrice de dimension m*n\n",
    "    Ones=np.ones((m)).reshape(m,1) #crée une matrice de dimension m*1 remplie de 1\n",
    "    X=np.hstack([Ones,X]) # Concatenation horizontale des matrice X et Ones pour donner lieu à une nouvelle matrice X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez la fonction Stand_Trans au vecteur X\n",
    "X=Stand_Trans(X,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  3.]\n",
      " [ 1.  4.]\n",
      " [ 1.  5.]\n",
      " [ 1.  6.]\n",
      " [ 1.  7.]\n",
      " [ 1.  8.]\n",
      " [ 1.  9.]\n",
      " [ 1. 10.]\n",
      " [ 1. 11.]\n",
      " [ 1. 12.]\n",
      " [ 1. 13.]\n",
      " [ 1. 14.]\n",
      " [ 1. 15.]\n",
      " [ 1. 16.]\n",
      " [ 1. 17.]\n",
      " [ 1. 18.]\n",
      " [ 1. 19.]\n",
      " [ 1. 20.]\n",
      " [ 1. 21.]\n",
      " [ 1. 22.]\n",
      " [ 1. 23.]\n",
      " [ 1. 24.]\n",
      " [ 1. 25.]\n",
      " [ 1. 26.]\n",
      " [ 1. 27.]\n",
      " [ 1. 28.]\n",
      " [ 1. 29.]\n",
      " [ 1. 30.]\n",
      " [ 1. 31.]\n",
      " [ 1. 32.]\n",
      " [ 1. 33.]\n",
      " [ 1. 34.]\n",
      " [ 1. 35.]\n",
      " [ 1. 36.]\n",
      " [ 1. 37.]\n",
      " [ 1. 38.]\n",
      " [ 1. 39.]\n",
      " [ 1. 40.]\n",
      " [ 1. 41.]\n",
      " [ 1. 42.]\n",
      " [ 1. 43.]\n",
      " [ 1. 44.]\n",
      " [ 1. 45.]\n",
      " [ 1. 46.]\n",
      " [ 1. 47.]\n",
      " [ 1. 48.]\n",
      " [ 1. 49.]\n",
      " [ 1. 50.]\n",
      " [ 1. 51.]\n",
      " [ 1. 52.]\n",
      " [ 1. 53.]\n",
      " [ 1. 54.]\n",
      " [ 1. 55.]\n",
      " [ 1. 56.]\n",
      " [ 1. 57.]\n",
      " [ 1. 58.]\n",
      " [ 1. 59.]\n",
      " [ 1. 60.]\n",
      " [ 1. 61.]\n",
      " [ 1. 62.]\n",
      " [ 1. 63.]\n",
      " [ 1. 64.]\n",
      " [ 1. 65.]\n",
      " [ 1. 66.]\n",
      " [ 1. 67.]\n",
      " [ 1. 68.]\n",
      " [ 1. 69.]\n",
      " [ 1. 70.]\n",
      " [ 1. 71.]\n",
      " [ 1. 72.]\n",
      " [ 1. 73.]\n",
      " [ 1. 74.]\n",
      " [ 1. 75.]\n",
      " [ 1. 76.]\n",
      " [ 1. 77.]\n",
      " [ 1. 78.]\n",
      " [ 1. 79.]\n",
      " [ 1. 80.]\n",
      " [ 1. 81.]\n",
      " [ 1. 82.]\n",
      " [ 1. 83.]\n",
      " [ 1. 84.]\n",
      " [ 1. 85.]\n",
      " [ 1. 86.]\n",
      " [ 1. 87.]\n",
      " [ 1. 88.]\n",
      " [ 1. 89.]\n",
      " [ 1. 90.]\n",
      " [ 1. 91.]\n",
      " [ 1. 92.]\n",
      " [ 1. 93.]\n",
      " [ 1. 94.]\n",
      " [ 1. 95.]\n",
      " [ 1. 96.]\n",
      " [ 1. 97.]\n",
      " [ 1. 98.]\n",
      " [ 1. 99.]]\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de la nouvelle matrice X\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez qu'à ce stade votre matrice $\\ X $ est de dimension $\\ 100 \\times 2 $ et que la première colonne colonne ne contient que des $\\ 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation normale\n",
    "Dans le cas de la régression linéaire ($\\ \\hat{Y}= X \\cdot \\theta $ ), l'équation normale donne une solution exacte à la problématique de minimisation de la fonction ***Loss*** lorque celle-ci correspond à une MSE (Mean Squared Error).<br>\n",
    "\\begin{equation}\n",
    "\\theta=(X^T \\times X)^{-1} \\times X^T \\times Y\n",
    "\\end{equation}<br>\n",
    "Pour calculer les $\\ \\theta_{j} $, on utilise les méthodes de calcul matriciel de la bibliothèque ***numpy***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 5.]\n"
     ]
    }
   ],
   "source": [
    "Theta=np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction = 507.00000000000006\n"
     ]
    }
   ],
   "source": [
    "X1 = [1 , 101]\n",
    "prediction = np.dot(X1, Theta)\n",
    "print('prediction =', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des paramètres $\\ \\theta_{j} $ obtenues avec l'équation normale sont optimales. Néanmoins, le calcul peut être long lorsque le nombre de variables est grand. La descente de gradient est une alternative à l'équation normale dans le cas linéaire lorsque le nombre de variables est grand. \n",
    "Par ailleurs, la descente de gradient s'applique à des modèles non-linéaires pour lesuqles il n'existe pas de solution exacte à la problamatique de minimisation de la ***loss***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procédure générique de descente de gradient\n",
    "La descente de gradient est une approche ittérative qui conssiste à mettre à jour les pramètres $\\ \\theta_{j} $ en effectuant un déplacement dans le sens opposé au gradient.<br>\n",
    "De façon générique, la mise à jour des paramètres $\\ \\theta_{j} $ s’effectue comme suit :<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^k=\\theta^{k-1}- \\eta \\times \\nabla_{\\theta}MSE \n",
    "\\end{equation}<br>\n",
    "\n",
    "Avec :<br>\n",
    "- $\\ \\theta $ : le vecteur des paramètres $\\ \\theta_{j} $\n",
    "- $\\ k $ : le numéro d'itération en cours\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ \\nabla_{\\theta}MSE$ : le vecteur gradient <br>\n",
    "\n",
    "*Remarque: l'initialisation de $\\ \\theta $ est arbitraire.*<br>\n",
    "\n",
    "Il existe de nombreuse variantes de la descente de gradient qui se différentient par la façon de calculer $\\ \\nabla_{\\theta}MSE$. <br>\n",
    "Nous pouvons donc écrire une fonction générique de descente de gradient dont un des arguments est lui-même une fonction qui précise la façons de calculer $\\ \\nabla_{\\theta}MSE$.<br>\n",
    "Ecrvivez la fonction ***Grad_Desc*** qui renvoie en sortie $\\ \\theta $ et qui prend comme arguments:<br>\n",
    "- $\\ X $: la matrice des données d'apprentissage\n",
    "- $\\ Y $: le vecteur des labels\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ K $ : le nombre total d'itérations\n",
    "- $\\ Grad $ : la fonction de calcul de $\\ \\nabla_{\\theta}MSE$ <br>\n",
    "\n",
    "*Remarque : dans un premier temps, faites comme si la fonction ***Grad*** était connue. Vous en préciserez le comportement dans un second temps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eta = learning_rate\n",
    "#K = nb_iterations\n",
    "#Grad = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc(X,Y,Eta,K,Grad_function): #algorithme de gradient\n",
    "    Theta = [7, 8] #random initialisation\n",
    "    for i in range(0, K):\n",
    "        if Grad_function == 'stochastic':\n",
    "            Grad = Grad_Stochastic(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        elif Grad_function == 'regular':\n",
    "            Grad = Grad_Regular(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        else :\n",
    "            print('Syntax Error :\"', Grad_function, '\"')\n",
    "            return\n",
    "        Theta = Theta - Eta * Grad #on fait un petit pas\n",
    "        print('i =', i, 'Theta =', Theta)\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient ordinaire\n",
    "Les composantes du vecteur gradient correpondent aux dérivées partielles de la fonction ***Loss*** par rapport à chaque paramètre $\\ \\theta_{j} $. Le vecteur gradient s'écrit comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}MSE=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{0}}\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{1}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{n}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Dans le cas de la descente de gradient ordinaire, la totalité du jeu d'entrainement est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous. <br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i=1}^{m} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= \\dfrac{2}{m} \\times X^T\\cdot(X\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Regular*** qui calcule le gradient ordinaire en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Regular(X,Y,Theta): #calcule le vecteur de gradient\n",
    "    m = len(Y)\n",
    "    Grad = 2/m * X.transpose().dot(X.dot(Theta) - Y)\n",
    "    #print('TYPE==================', type(Theta))\n",
    "    #print('TYPE==================', type(X))\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Regular***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [  307. 20196.]\n",
      "i = 0 Theta = [6.9693 5.9804]\n",
      "Grad = [ 106.9982 6930.2475]\n",
      "i = 1 Theta = [6.95860018 5.28737525]\n",
      "Grad = [  38.36735011 2378.09468457]\n",
      "i = 2 Theta = [6.95476344 5.04956578]\n",
      "Grad = [ 14.81653926 816.02006845]\n",
      "i = 3 Theta = [6.95328179 4.96796377]\n",
      "Grad = [  6.73497728 279.99300576]\n",
      "i = 4 Theta = [6.95260829 4.93996447]\n",
      "Grad = [ 3.96169952 96.0549226 ]\n",
      "i = 5 Theta = [6.95221212 4.93035898]\n",
      "Grad = [ 3.00996345 32.9364341 ]\n",
      "i = 6 Theta = [6.95191113 4.92706534]\n",
      "Grad = [ 2.68329076 11.27727919]\n",
      "i = 7 Theta = [6.9516428  4.92593761]\n",
      "Grad = [2.57110904 3.84492537]\n",
      "i = 8 Theta = [6.95138569 4.92555312]\n",
      "Grad = [2.53253006 1.2945089 ]\n",
      "i = 9 Theta = [6.95113243 4.92542367]\n",
      "Grad = [2.51920791 0.41933286]\n",
      "i = 10 Theta = [6.95088051 4.92538173]\n",
      "Grad = [2.51455267 0.11901681]\n",
      "i = 11 Theta = [6.95062906 4.92536983]\n",
      "Grad = [2.5128715 0.0159644]\n",
      "i = 12 Theta = [6.95037777 4.92536824]\n",
      "Grad = [ 2.51221088 -0.01939685]\n",
      "i = 13 Theta = [6.95012655 4.92537018]\n",
      "Grad = [ 2.51190046 -0.03152983]\n",
      "i = 14 Theta = [6.94987536 4.92537333]\n",
      "Grad = [ 2.51171023 -0.035692  ]\n",
      "i = 15 Theta = [6.94962419 4.9253769 ]\n",
      "Grad = [ 2.51156124 -0.037119  ]\n",
      "i = 16 Theta = [6.94937303 4.92538061]\n",
      "Grad = [ 2.5114264  -0.03760741]\n",
      "i = 17 Theta = [6.94912189 4.92538437]\n",
      "Grad = [ 2.51129643 -0.03777374]\n",
      "i = 18 Theta = [6.94887076 4.92538815]\n",
      "Grad = [ 2.51116813 -0.03782956]\n",
      "i = 19 Theta = [6.94861964 4.92539193]\n",
      "Grad = [ 2.51104041 -0.03784745]\n",
      "i = 20 Theta = [6.94836854 4.92539572]\n",
      "Grad = [ 2.51091289 -0.03785233]\n",
      "i = 21 Theta = [6.94811745 4.9253995 ]\n",
      "Grad = [ 2.51078545 -0.03785274]\n",
      "i = 22 Theta = [6.94786637 4.92540329]\n",
      "Grad = [ 2.51065803 -0.03785162]\n",
      "i = 23 Theta = [6.9476153  4.92540707]\n",
      "Grad = [ 2.51053063 -0.03784998]\n",
      "i = 24 Theta = [6.94736425 4.92541086]\n",
      "Grad = [ 2.51040324 -0.03784815]\n",
      "i = 25 Theta = [6.94711321 4.92541464]\n",
      "Grad = [ 2.51027586 -0.03784626]\n",
      "i = 26 Theta = [6.94686218 4.92541843]\n",
      "Grad = [ 2.51014848 -0.03784435]\n",
      "i = 27 Theta = [6.94661117 4.92542221]\n",
      "Grad = [ 2.51002111 -0.03784244]\n",
      "i = 28 Theta = [6.94636017 4.92542599]\n",
      "Grad = [ 2.50989374 -0.03784052]\n",
      "i = 29 Theta = [6.94610918 4.92542978]\n",
      "Grad = [ 2.50976639 -0.0378386 ]\n",
      "i = 30 Theta = [6.9458582  4.92543356]\n",
      "Grad = [ 2.50963904 -0.03783668]\n",
      "i = 31 Theta = [6.94560724 4.92543735]\n",
      "Grad = [ 2.50951169 -0.03783476]\n",
      "i = 32 Theta = [6.94535628 4.92544113]\n",
      "Grad = [ 2.50938435 -0.03783284]\n",
      "i = 33 Theta = [6.94510535 4.92544491]\n",
      "Grad = [ 2.50925702 -0.03783092]\n",
      "i = 34 Theta = [6.94485442 4.9254487 ]\n",
      "Grad = [ 2.5091297 -0.037829 ]\n",
      "i = 35 Theta = [6.94460351 4.92545248]\n",
      "Grad = [ 2.50900238 -0.03782708]\n",
      "i = 36 Theta = [6.94435261 4.92545626]\n",
      "Grad = [ 2.50887506 -0.03782516]\n",
      "i = 37 Theta = [6.94410172 4.92546004]\n",
      "Grad = [ 2.50874776 -0.03782324]\n",
      "i = 38 Theta = [6.94385085 4.92546383]\n",
      "Grad = [ 2.50862046 -0.03782132]\n",
      "i = 39 Theta = [6.94359998 4.92546761]\n",
      "Grad = [ 2.50849317 -0.0378194 ]\n",
      "i = 40 Theta = [6.94334913 4.92547139]\n",
      "Grad = [ 2.50836588 -0.03781748]\n",
      "i = 41 Theta = [6.9430983  4.92547517]\n",
      "Grad = [ 2.5082386  -0.03781556]\n",
      "i = 42 Theta = [6.94284747 4.92547895]\n",
      "Grad = [ 2.50811133 -0.03781365]\n",
      "i = 43 Theta = [6.94259666 4.92548273]\n",
      "Grad = [ 2.50798406 -0.03781173]\n",
      "i = 44 Theta = [6.94234586 4.92548652]\n",
      "Grad = [ 2.5078568  -0.03780981]\n",
      "i = 45 Theta = [6.94209508 4.9254903 ]\n",
      "Grad = [ 2.50772954 -0.03780789]\n",
      "i = 46 Theta = [6.94184431 4.92549408]\n",
      "Grad = [ 2.5076023  -0.03780597]\n",
      "i = 47 Theta = [6.94159354 4.92549786]\n",
      "Grad = [ 2.50747505 -0.03780405]\n",
      "i = 48 Theta = [6.9413428  4.92550164]\n",
      "Grad = [ 2.50734782 -0.03780213]\n",
      "i = 49 Theta = [6.94109206 4.92550542]\n",
      "Grad = [ 2.50722059 -0.03780022]\n",
      "i = 50 Theta = [6.94084134 4.9255092 ]\n",
      "Grad = [ 2.50709337 -0.0377983 ]\n",
      "i = 51 Theta = [6.94059063 4.92551298]\n",
      "Grad = [ 2.50696615 -0.03779638]\n",
      "i = 52 Theta = [6.94033993 4.92551676]\n",
      "Grad = [ 2.50683894 -0.03779446]\n",
      "i = 53 Theta = [6.94008925 4.92552054]\n",
      "Grad = [ 2.50671174 -0.03779254]\n",
      "i = 54 Theta = [6.93983858 4.92552432]\n",
      "Grad = [ 2.50658455 -0.03779063]\n",
      "i = 55 Theta = [6.93958792 4.9255281 ]\n",
      "Grad = [ 2.50645736 -0.03778871]\n",
      "i = 56 Theta = [6.93933728 4.92553187]\n",
      "Grad = [ 2.50633017 -0.03778679]\n",
      "i = 57 Theta = [6.93908664 4.92553565]\n",
      "Grad = [ 2.506203   -0.03778487]\n",
      "i = 58 Theta = [6.93883602 4.92553943]\n",
      "Grad = [ 2.50607583 -0.03778296]\n",
      "i = 59 Theta = [6.93858541 4.92554321]\n",
      "Grad = [ 2.50594866 -0.03778104]\n",
      "i = 60 Theta = [6.93833482 4.92554699]\n",
      "Grad = [ 2.5058215  -0.03777912]\n",
      "i = 61 Theta = [6.93808424 4.92555077]\n",
      "Grad = [ 2.50569435 -0.03777721]\n",
      "i = 62 Theta = [6.93783367 4.92555454]\n",
      "Grad = [ 2.50556721 -0.03777529]\n",
      "i = 63 Theta = [6.93758311 4.92555832]\n",
      "Grad = [ 2.50544007 -0.03777337]\n",
      "i = 64 Theta = [6.93733257 4.9255621 ]\n",
      "Grad = [ 2.50531294 -0.03777146]\n",
      "i = 65 Theta = [6.93708204 4.92556588]\n",
      "Grad = [ 2.50518581 -0.03776954]\n",
      "i = 66 Theta = [6.93683152 4.92556965]\n",
      "Grad = [ 2.5050587  -0.03776762]\n",
      "i = 67 Theta = [6.93658101 4.92557343]\n",
      "Grad = [ 2.50493158 -0.03776571]\n",
      "i = 68 Theta = [6.93633052 4.92557721]\n",
      "Grad = [ 2.50480448 -0.03776379]\n",
      "i = 69 Theta = [6.93608004 4.92558098]\n",
      "Grad = [ 2.50467738 -0.03776187]\n",
      "i = 70 Theta = [6.93582957 4.92558476]\n",
      "Grad = [ 2.50455029 -0.03775996]\n",
      "i = 71 Theta = [6.93557912 4.92558854]\n",
      "Grad = [ 2.5044232  -0.03775804]\n",
      "i = 72 Theta = [6.93532867 4.92559231]\n",
      "Grad = [ 2.50429612 -0.03775613]\n",
      "i = 73 Theta = [6.93507824 4.92559609]\n",
      "Grad = [ 2.50416904 -0.03775421]\n",
      "i = 74 Theta = [6.93482783 4.92559986]\n",
      "Grad = [ 2.50404198 -0.03775229]\n",
      "i = 75 Theta = [6.93457742 4.92560364]\n",
      "Grad = [ 2.50391492 -0.03775038]\n",
      "i = 76 Theta = [6.93432703 4.92560741]\n",
      "Grad = [ 2.50378786 -0.03774846]\n",
      "i = 77 Theta = [6.93407665 4.92561119]\n",
      "Grad = [ 2.50366082 -0.03774655]\n",
      "i = 78 Theta = [6.93382629 4.92561496]\n",
      "Grad = [ 2.50353377 -0.03774463]\n",
      "i = 79 Theta = [6.93357593 4.92561874]\n",
      "Grad = [ 2.50340674 -0.03774272]\n",
      "i = 80 Theta = [6.93332559 4.92562251]\n",
      "Grad = [ 2.50327971 -0.0377408 ]\n",
      "i = 81 Theta = [6.93307526 4.92562628]\n",
      "Grad = [ 2.50315269 -0.03773889]\n",
      "i = 82 Theta = [6.93282495 4.92563006]\n",
      "Grad = [ 2.50302567 -0.03773697]\n",
      "i = 83 Theta = [6.93257465 4.92563383]\n",
      "Grad = [ 2.50289866 -0.03773506]\n",
      "i = 84 Theta = [6.93232436 4.92563761]\n",
      "Grad = [ 2.50277166 -0.03773314]\n",
      "i = 85 Theta = [6.93207408 4.92564138]\n",
      "Grad = [ 2.50264466 -0.03773123]\n",
      "i = 86 Theta = [6.93182381 4.92564515]\n",
      "Grad = [ 2.50251767 -0.03772931]\n",
      "i = 87 Theta = [6.93157356 4.92564892]\n",
      "Grad = [ 2.50239069 -0.0377274 ]\n",
      "i = 88 Theta = [6.93132332 4.9256527 ]\n",
      "Grad = [ 2.50226371 -0.03772548]\n",
      "i = 89 Theta = [6.9310731  4.92565647]\n",
      "Grad = [ 2.50213674 -0.03772357]\n",
      "i = 90 Theta = [6.93082288 4.92566024]\n",
      "Grad = [ 2.50200978 -0.03772166]\n",
      "i = 91 Theta = [6.93057268 4.92566401]\n",
      "Grad = [ 2.50188282 -0.03771974]\n",
      "i = 92 Theta = [6.93032249 4.92566779]\n",
      "Grad = [ 2.50175587 -0.03771783]\n",
      "i = 93 Theta = [6.93007232 4.92567156]\n",
      "Grad = [ 2.50162893 -0.03771591]\n",
      "i = 94 Theta = [6.92982216 4.92567533]\n",
      "Grad = [ 2.50150199 -0.037714  ]\n",
      "i = 95 Theta = [6.92957201 4.9256791 ]\n",
      "Grad = [ 2.50137506 -0.03771209]\n",
      "i = 96 Theta = [6.92932187 4.92568287]\n",
      "Grad = [ 2.50124813 -0.03771017]\n",
      "i = 97 Theta = [6.92907174 4.92568664]\n",
      "Grad = [ 2.50112121 -0.03770826]\n",
      "i = 98 Theta = [6.92882163 4.92569041]\n",
      "Grad = [ 2.5009943  -0.03770635]\n",
      "i = 99 Theta = [6.92857153 4.92569419]\n"
     ]
    }
   ],
   "source": [
    "Theta = Grad_Desc(X,Y,0.0001,100,'regular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_regular = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_regular = np.dot(X1, Theta)\n",
    "print('prediction_regular =', prediction_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient stochastique\n",
    "Dans le cas de la descente de gradient stochastique, seule une instance $\\ X_{i} $ du jeu d'entrainement, tirée au hasard à chaque itération, est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous.<br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=2 \\times x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Pour une instance $\\ i$ prise au hasard, le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= 2 \\times X_{i}\\cdot(X_{i}\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Stochastic*** qui calcule le gradient stochastique en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # à utiliser pour le tirage aléatoire d'une instance Xi à chaque itération\n",
    "def Grad_Stochastic(X,Y,Theta):\n",
    "    i = random.randrange(0, 99, 1)\n",
    "    #Xi = Xi.reshape(2, 1)\n",
    "    #Theta = np.reshape(Theta, (2, 1))\n",
    "    Grad = 2 * X[i].dot(X[i].dot(Theta) - Y[i]) #impossible avec X.shape(2,) et Y(100,) donc j'ai mis Y[i]\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient stochastique sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Stochastic***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [  394. 25216.]\n",
      "i = 0 Theta = [6.9606 5.4784]\n",
      "Grad = [ 20.446 224.906]\n",
      "i = 1 Theta = [6.9585554 5.4559094]\n",
      "Grad = [  98.3635344 9541.2628368]\n",
      "i = 2 Theta = [6.94871905 4.50178312]\n",
      "Grad = [0.92953419 8.36580768]\n",
      "i = 3 Theta = [6.94862609 4.50094654]\n",
      "Grad = [  -77.93615756 -6858.38186498]\n",
      "i = 4 Theta = [6.95641971 5.18678472]\n",
      "Grad = [  43.90765883 3995.59695361]\n",
      "i = 5 Theta = [6.95202894 4.78722503]\n",
      "Grad = [  -5.41574019 -194.96664693]\n",
      "i = 6 Theta = [6.95257052 4.80672169]\n",
      "Grad = [  -24.4983979 -2180.3574131]\n",
      "i = 7 Theta = [6.95502036 5.02475743]\n",
      "Grad = [ 10.70227856 171.23645696]\n",
      "i = 8 Theta = [6.95395013 5.00763379]\n",
      "Grad = [ 10.80868712 637.71254028]\n",
      "i = 9 Theta = [6.95286926 4.94386253]\n",
      "Grad = [  8.55843931 102.70127174]\n",
      "i = 10 Theta = [6.95201342 4.93359241]\n",
      "Grad = [  6.71646231 161.19509546]\n",
      "i = 11 Theta = [6.95134177 4.9174729 ]\n",
      "Grad = [ 8.08708726 88.95795983]\n",
      "i = 12 Theta = [6.95053306 4.9085771 ]\n",
      "Grad = [  -4.36090624 -340.15068641]\n",
      "i = 13 Theta = [6.95096915 4.94259217]\n",
      "Grad = [  -1.34999659 -132.29966588]\n",
      "i = 14 Theta = [6.95110415 4.95582214]\n",
      "Grad = [  1.95019269 175.51734228]\n",
      "i = 15 Theta = [6.95090913 4.9382704 ]\n",
      "Grad = [  5.70420555 193.94298873]\n",
      "i = 16 Theta = [6.95033871 4.9188761 ]\n",
      "Grad = [  3.73526121 141.93992581]\n",
      "i = 17 Theta = [6.94996519 4.90468211]\n",
      "Grad = [  -4.77902472 -367.98490326]\n",
      "i = 18 Theta = [6.95044309 4.9414806 ]\n",
      "Grad = [  1.4740926  106.13466692]\n",
      "i = 19 Theta = [6.95029568 4.93086713]\n",
      "Grad = [  -2.54332462 -228.89921598]\n",
      "i = 20 Theta = [6.95055001 4.95375706]\n",
      "Grad = [  7.31149511 204.72186295]\n",
      "i = 21 Theta = [6.94981886 4.93328487]\n",
      "Grad = [ 1.49353119 94.09246472]\n",
      "i = 22 Theta = [6.94966951 4.92387562]\n",
      "Grad = [  4.57063258 159.97214019]\n",
      "i = 23 Theta = [6.94921245 4.90787841]\n",
      "Grad = [  -7.42043436 -697.52082952]\n",
      "i = 24 Theta = [6.94995449 4.97763049]\n",
      "Grad = [  6.54448266 490.83619945]\n",
      "i = 25 Theta = [6.94930004 4.92854687]\n",
      "Grad = [  -2.10552555 -176.86414588]\n",
      "i = 26 Theta = [6.94951059 4.94623329]\n",
      "Grad = [  2.26414778 160.7544922 ]\n",
      "i = 27 Theta = [6.94928418 4.93015784]\n",
      "Grad = [  4.03182663 169.33671863]\n",
      "i = 28 Theta = [6.948881   4.91322416]\n",
      "Grad = [  -5.72188835 -514.96995149]\n",
      "i = 29 Theta = [6.94945318 4.96472116]\n",
      "Grad = [  5.80656092 336.78053335]\n",
      "i = 30 Theta = [6.94887253 4.93104311]\n",
      "Grad = [  6.58781417 158.10754015]\n",
      "i = 31 Theta = [6.94821375 4.91523235]\n",
      "Grad = [  -5.87035493 -545.94300831]\n",
      "i = 32 Theta = [6.94880078 4.96982665]\n",
      "Grad = [  4.64743926 404.32721533]\n",
      "i = 33 Theta = [6.94833604 4.92939393]\n",
      "Grad = [  6.36636867 159.15921677]\n",
      "i = 34 Theta = [6.9476994  4.91347801]\n",
      "Grad = [  -1.52550385 -100.68325419]\n",
      "i = 35 Theta = [6.94785195 4.92354634]\n",
      "Grad = [  3.4735961  145.89103601]\n",
      "i = 36 Theta = [6.94750459 4.90895723]\n",
      "Grad = [  5.70704186 131.26196272]\n",
      "i = 37 Theta = [6.94693389 4.89583104]\n",
      "Grad = [  5.51877128 115.89419683]\n",
      "i = 38 Theta = [6.94638201 4.88424162]\n",
      "Grad = [ 9.42973049 18.85946097]\n",
      "i = 39 Theta = [6.94543904 4.88235567]\n",
      "Grad = [ 0.71462033 27.87019298]\n",
      "i = 40 Theta = [6.94536758 4.87956865]\n",
      "Grad = [  -9.13741804 -721.85602547]\n",
      "i = 41 Theta = [6.94628132 4.95175425]\n",
      "Grad = [ 9.69957965 19.3991593 ]\n",
      "i = 42 Theta = [6.94531136 4.94981434]\n",
      "Grad = [  5.77539838 236.79133344]\n",
      "i = 43 Theta = [6.94473382 4.9261352 ]\n",
      "Grad = [ 8.26444213 90.9088634 ]\n",
      "i = 44 Theta = [6.94390738 4.91704432]\n",
      "Grad = [ -0.89642396 -58.26755748]\n",
      "i = 45 Theta = [6.94399702 4.92287107]\n",
      "Grad = [  -2.45263423 -196.21073832]\n",
      "i = 46 Theta = [6.94424228 4.94249215]\n",
      "Grad = [ 9.31340604 46.56703018]\n",
      "i = 47 Theta = [6.94331094 4.93783544]\n",
      "Grad = [  5.28644475 195.59845574]\n",
      "i = 48 Theta = [6.9427823 4.9182756]\n",
      "Grad = [  2.20347086 103.56313042]\n",
      "i = 49 Theta = [6.94256195 4.90791929]\n",
      "Grad = [ -0.61207755 -34.88842032]\n",
      "i = 50 Theta = [6.94262316 4.91140813]\n",
      "Grad = [  -3.75790204 -289.35845704]\n",
      "i = 51 Theta = [6.94299895 4.94034397]\n",
      "Grad = [  7.61906888 144.76230871]\n",
      "i = 52 Theta = [6.94223704 4.92586774]\n",
      "Grad = [  3.06430638 140.95809359]\n",
      "i = 53 Theta = [6.94193061 4.91177193]\n",
      "Grad = [  6.88410694 117.02981804]\n",
      "i = 54 Theta = [6.9412422  4.90006895]\n",
      "Grad = [  -6.90593179 -580.09827062]\n",
      "i = 55 Theta = [6.94193279 4.95807878]\n",
      "Grad = [  6.44632541 264.29934164]\n",
      "i = 56 Theta = [6.94128816 4.93164884]\n",
      "Grad = [  8.10544627 105.37080148]\n",
      "i = 57 Theta = [6.94047762 4.92111176]\n",
      "Grad = [ 7.98763757 95.8516508 ]\n",
      "i = 58 Theta = [6.93967885 4.9115266 ]\n",
      "Grad = [ 8.28683648 74.58152835]\n",
      "i = 59 Theta = [6.93885017 4.90406845]\n",
      "Grad = [ 0.47640805 23.34399451]\n",
      "i = 60 Theta = [6.93880253 4.90173405]\n",
      "Grad = [  -2.89696888 -188.3029775 ]\n",
      "i = 61 Theta = [6.93909222 4.92056434]\n",
      "Grad = [ -1.24280734 -86.99651377]\n",
      "i = 62 Theta = [6.9392165 4.929264 ]\n",
      "Grad = [ -1.01491164 -78.14819665]\n",
      "i = 63 Theta = [6.939318   4.93707882]\n",
      "Grad = [  2.70562095 154.22039411]\n",
      "i = 64 Theta = [6.93904743 4.92165678]\n",
      "Grad = [  -4.6937448  -436.51826597]\n",
      "i = 65 Theta = [6.93951681 4.9653086 ]\n",
      "Grad = [  6.61804226 311.04798628]\n",
      "i = 66 Theta = [6.938855  4.9342038]\n",
      "Grad = [  2.50853605 140.47801899]\n",
      "i = 67 Theta = [6.93860415 4.920156  ]\n",
      "Grad = [ 0.45561655 26.88137621]\n",
      "i = 68 Theta = [6.93855859 4.91746786]\n",
      "Grad = [ -0.19180336 -11.70000489]\n",
      "i = 69 Theta = [6.93857777 4.91863786]\n",
      "Grad = [ 1.41549349 73.60566161]\n",
      "i = 70 Theta = [6.93843622 4.9112773 ]\n",
      "Grad = [  -3.07664194 -224.59486148]\n",
      "i = 71 Theta = [6.93874388 4.93373678]\n",
      "Grad = [  6.43180058 167.2268152 ]\n",
      "i = 72 Theta = [6.9381007 4.9170141]\n",
      "Grad = [  -5.22723177 -475.67809119]\n",
      "i = 73 Theta = [6.93862343 4.96458191]\n",
      "Grad = [  5.06038696 344.10631316]\n",
      "i = 74 Theta = [6.93811739 4.93017128]\n",
      "Grad = [ 8.20034553 98.40414631]\n",
      "i = 75 Theta = [6.93729735 4.92033087]\n",
      "Grad = [  -1.91643704 -141.81634082]\n",
      "i = 76 Theta = [6.937489  4.9345125]\n",
      "Grad = [  4.50500305 184.70512503]\n",
      "i = 77 Theta = [6.9370385  4.91604199]\n",
      "Grad = [ -0.36880045 -22.49682744]\n",
      "i = 78 Theta = [6.93707538 4.91829167]\n",
      "Grad = [  -4.01626519 -341.38254128]\n",
      "i = 79 Theta = [6.937477   4.95242993]\n",
      "Grad = [  8.54299191 119.60188672]\n",
      "i = 80 Theta = [6.9366227  4.94046974]\n",
      "Grad = [  6.18236906 191.65344097]\n",
      "i = 81 Theta = [6.93600447 4.92130439]\n",
      "Grad = [0.11375358 7.05272172]\n",
      "i = 82 Theta = [6.93599309 4.92059912]\n",
      "Grad = [  6.37834747 140.32364431]\n",
      "i = 83 Theta = [6.93535526 4.90656676]\n",
      "Grad = [  6.13338074 122.66761483]\n",
      "i = 84 Theta = [6.93474192 4.89429999]\n",
      "Grad = [  -9.36791722 -852.48046709]\n",
      "i = 85 Theta = [6.93567871 4.97954804]\n",
      "Grad = [  7.86706543 385.48620604]\n",
      "i = 86 Theta = [6.934892   4.94099942]\n",
      "Grad = [  3.4977214 188.8769556]\n",
      "i = 87 Theta = [6.93454223 4.92211172]\n",
      "Grad = [ -0.87949752 -60.6853291 ]\n",
      "i = 88 Theta = [6.93463018 4.92818026]\n",
      "Grad = [  4.26732046 166.42549794]\n",
      "i = 89 Theta = [6.93420345 4.91153771]\n",
      "Grad = [ 2.26064977 97.20794031]\n",
      "i = 90 Theta = [6.93397739 4.90181691]\n",
      "Grad = [  -8.78683161 -834.74900258]\n",
      "i = 91 Theta = [6.93485607 4.98529181]\n",
      "Grad = [  8.92838824 285.70842359]\n",
      "i = 92 Theta = [6.93396323 4.95672097]\n",
      "Grad = [ 9.60825229 28.82475687]\n",
      "i = 93 Theta = [6.9330024 4.9538385]\n",
      "Grad = [  6.17308449 246.92337962]\n",
      "i = 94 Theta = [6.9323851  4.92914616]\n",
      "Grad = [  8.0225703  104.29341392]\n",
      "i = 95 Theta = [6.93158284 4.91871682]\n",
      "Grad = [ -0.86621452 -57.17015847]\n",
      "i = 96 Theta = [6.93166946 4.92443383]\n",
      "Grad = [  -4.94762991 -484.86773087]\n",
      "i = 97 Theta = [6.93216422 4.97292061]\n",
      "Grad = [  5.47746655 443.67479086]\n",
      "i = 98 Theta = [6.93161648 4.92855313]\n",
      "Grad = [ 1.28960814 77.3764881 ]\n",
      "i = 99 Theta = [6.93148752 4.92081548]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.93148752, 4.92081548])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grad_Desc(X,Y,0.0001,100,\"stochastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_stochastic = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_stochastic = np.dot(X1, Theta)\n",
    "print('prediction_stochastic =', prediction_stochastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la descente de gradient mini-batch, un sous-ensemble du jeu d'entrainement, constitué de $\\ q $ exemples tirés aléatoirement, est utilisé à chaque itération pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$. Le nombre $\\ q $  est un hyperparamètre fixé par l'utilisateur avec $\\ q \\le m$.<br>\n",
    "\n",
    "La formule pour le calcul $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ devient:\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i\\in \\mathscr{E}} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Avec $\\mathscr{E} $ l'ensemble des $\\ q $ exemples tirés aléatoirement (cet ensemble est réactualisé à cahque itération).\n",
    "\n",
    "Ecrivez la fonction ***Grad_Batch*** qui calcule le gradient mini_batch en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Already_choose(j_history, j): #renvoie 1 si j a déjà été tiré, 0 sinon\n",
    "    count = 0\n",
    "    for i in j_history:\n",
    "        if (j_history[count] == j):\n",
    "            return (1)\n",
    "        count+=1\n",
    "    return (0)\n",
    "\n",
    "def examples(q): #retourne une liste des exemples tirés, jamais 2 fois le même exemple\n",
    "    j_history = np.zeros(q) #créer un tableau de 0 de taille q pour mettre l'historique des j tirés\n",
    "    i = 0\n",
    "    while i < q: #on parcours le nombre d'exemples demandé\n",
    "        j = random.randrange(0, 99, 1) #on tire au hasard un exemple\n",
    "        if (Already_choose(j_history, j) == 0): #s'il a pas déjà été tiré\n",
    "            j_history[i] = j #on le rentre dans notre liste d'exemples\n",
    "            i+=1 #on passe au suivant\n",
    "    return(j_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Batch(X,Y,Theta,q):\n",
    "    j_history = examples(q)\n",
    "    count, sum = 0, 0\n",
    "    i = 0\n",
    "    m = len(Y) \n",
    "    while count < q:\n",
    "        i = int(j_history[count])\n",
    "        #sum = sum + x[i][j].dot(X[i].dot(Theta) - Y[i]) #???? c'est quoi x[i][j]\n",
    "        sum = sum + X[i].dot(X[i].dot(Theta) - Y[i]) #???? \n",
    "        count+=1\n",
    "    Grad = 2/m * sum\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Batch***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$\n",
    "- $\\ q=10$\n",
    "\n",
    "*Remarque : pensez à adapter la fonction ***Grad_Desc*** en ***Grad_Desc_Batch***  pour tenir compte de l'hyperparamètre $\\ q $ spécifique à l'approche mini-batch. Dans la nouvelle fonction ***Grad_Desc_Batch***, le paramètre ***Grad*** prend par défaut la valeur ***Grad_Batch***.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc_Batch(X,Y,Eta,K,q,Grad=Grad_Batch):\n",
    "    Theta = [7, 12] #random initialisation\n",
    "    for i in range(0, K):\n",
    "        Grad = Grad_Batch(X,Y,Theta,q)\n",
    "        Theta = Theta - Eta * Grad #on fait un petit pas\n",
    "        print('i =', i, 'Theta =', Theta)\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 Theta = [ 6.99206  11.443192]\n",
      "i = 1 Theta = [ 6.98760456 11.18827515]\n",
      "i = 2 Theta = [ 6.98304925 10.9498019 ]\n",
      "i = 3 Theta = [ 6.97679749 10.4799818 ]\n",
      "i = 4 Theta = [ 6.9709111  10.10164914]\n",
      "i = 5 Theta = [6.96615898 9.78654768]\n",
      "i = 6 Theta = [6.96203895 9.54354652]\n",
      "i = 7 Theta = [6.95750521 9.24904339]\n",
      "i = 8 Theta = [6.95464418 9.08994061]\n",
      "i = 9 Theta = [6.94989074 8.73661637]\n",
      "i = 10 Theta = [6.94727326 8.60126549]\n",
      "i = 11 Theta = [6.94458861 8.45078892]\n",
      "i = 12 Theta = [6.94043159 8.14042126]\n",
      "i = 13 Theta = [6.93649518 7.85491796]\n",
      "i = 14 Theta = [6.93414107 7.73972772]\n",
      "i = 15 Theta = [6.93158759 7.59597514]\n",
      "i = 16 Theta = [6.92821803 7.36104554]\n",
      "i = 17 Theta = [6.92539009 7.14943702]\n",
      "i = 18 Theta = [6.92251022 6.94545722]\n",
      "i = 19 Theta = [6.91990602 6.76566006]\n",
      "i = 20 Theta = [6.91738513 6.56616344]\n",
      "i = 21 Theta = [6.91579267 6.45802715]\n",
      "i = 22 Theta = [6.9142509  6.35602383]\n",
      "i = 23 Theta = [6.9130678  6.28562557]\n",
      "i = 24 Theta = [6.91151935 6.18100304]\n",
      "i = 25 Theta = [6.910285   6.10353073]\n",
      "i = 26 Theta = [6.90911857 6.03090602]\n",
      "i = 27 Theta = [6.90806783 5.95986419]\n",
      "i = 28 Theta = [6.90702517 5.88843927]\n",
      "i = 29 Theta = [6.90607235 5.83273474]\n",
      "i = 30 Theta = [6.90511651 5.77012452]\n",
      "i = 31 Theta = [6.90436226 5.72411836]\n",
      "i = 32 Theta = [6.90345026 5.66819274]\n",
      "i = 33 Theta = [6.90271607 5.6188974 ]\n",
      "i = 34 Theta = [6.9019657  5.56732129]\n",
      "i = 35 Theta = [6.90130375 5.52495901]\n",
      "i = 36 Theta = [6.90062827 5.48524576]\n",
      "i = 37 Theta = [6.90007897 5.4553205 ]\n",
      "i = 38 Theta = [6.89959759 5.42975177]\n",
      "i = 39 Theta = [6.89904664 5.39620077]\n",
      "i = 40 Theta = [6.8985065  5.35825923]\n",
      "i = 41 Theta = [6.89801086 5.32696202]\n",
      "i = 42 Theta = [6.89776512 5.31771059]\n",
      "i = 43 Theta = [6.89733865 5.29000307]\n",
      "i = 44 Theta = [6.89696404 5.26612062]\n",
      "i = 45 Theta = [6.89663936 5.24827513]\n",
      "i = 46 Theta = [6.89628025 5.22588761]\n",
      "i = 47 Theta = [6.89593746 5.20534348]\n",
      "i = 48 Theta = [6.89566541 5.19101654]\n",
      "i = 49 Theta = [6.89533942 5.16927205]\n",
      "i = 50 Theta = [6.8950455  5.14900556]\n",
      "i = 51 Theta = [6.89481021 5.13700837]\n",
      "i = 52 Theta = [6.89459065 5.12529841]\n",
      "i = 53 Theta = [6.894383   5.11415642]\n",
      "i = 54 Theta = [6.8941568  5.09992582]\n",
      "i = 55 Theta = [6.893948   5.08733562]\n",
      "i = 56 Theta = [6.89375161 5.07516935]\n",
      "i = 57 Theta = [6.89358969 5.06737311]\n",
      "i = 58 Theta = [6.89342619 5.05819988]\n",
      "i = 59 Theta = [6.893265   5.04883396]\n",
      "i = 60 Theta = [6.89313032 5.04343281]\n",
      "i = 61 Theta = [6.89298642 5.03555361]\n",
      "i = 62 Theta = [6.89284916 5.02762707]\n",
      "i = 63 Theta = [6.89271992 5.01988935]\n",
      "i = 64 Theta = [6.89260047 5.01317664]\n",
      "i = 65 Theta = [6.89248538 5.00550041]\n",
      "i = 66 Theta = [6.8923825  5.00073822]\n",
      "i = 67 Theta = [6.89228387 4.99547597]\n",
      "i = 68 Theta = [6.89218994 4.99144832]\n",
      "i = 69 Theta = [6.89210091 4.9870235 ]\n",
      "i = 70 Theta = [6.89201397 4.98349268]\n",
      "i = 71 Theta = [6.89193551 4.97917136]\n",
      "i = 72 Theta = [6.8918587  4.97537754]\n",
      "i = 73 Theta = [6.89177943 4.97267609]\n",
      "i = 74 Theta = [6.89170679 4.96985673]\n",
      "i = 75 Theta = [6.89164    4.96680007]\n",
      "i = 76 Theta = [6.89157079 4.96418192]\n",
      "i = 77 Theta = [6.89151637 4.96125131]\n",
      "i = 78 Theta = [6.89144876 4.95919729]\n",
      "i = 79 Theta = [6.89139002 4.9571859 ]\n",
      "i = 80 Theta = [6.89134802 4.95488668]\n",
      "i = 81 Theta = [6.89130514 4.95309235]\n",
      "i = 82 Theta = [6.8912577  4.95146608]\n",
      "i = 83 Theta = [6.89120782 4.9498453 ]\n",
      "i = 84 Theta = [6.8911434  4.94832862]\n",
      "i = 85 Theta = [6.89108051 4.94692186]\n",
      "i = 86 Theta = [6.89103205 4.945721  ]\n",
      "i = 87 Theta = [6.89096191 4.94418373]\n",
      "i = 88 Theta = [6.89091678 4.94257472]\n",
      "i = 89 Theta = [6.89088282 4.94174727]\n",
      "i = 90 Theta = [6.89084442 4.94083276]\n",
      "i = 91 Theta = [6.89080565 4.93995294]\n",
      "i = 92 Theta = [6.89074879 4.93914359]\n",
      "i = 93 Theta = [6.89069564 4.93784113]\n",
      "i = 94 Theta = [6.8906401  4.93649385]\n",
      "i = 95 Theta = [6.89059957 4.93579011]\n",
      "i = 96 Theta = [6.89057046 4.93537509]\n",
      "i = 97 Theta = [6.89054735 4.93478483]\n",
      "i = 98 Theta = [6.89051332 4.93453472]\n",
      "i = 99 Theta = [6.89046029 4.93341516]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.89046029, 4.93341516])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grad_Desc_Batch(X,Y,0.0001,100,10,Grad=Grad_Batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_stochastic = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_batch = np.dot(X1, Theta)\n",
    "print('prediction_stochastic =', prediction_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests sur jeu de donnée plus conséquent\n",
    "Vous allez maintenant tester votre code sur un jeu de données plus conséquent et comparer vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***.<br>\n",
    "L'objectif est d'entrainer un régresseur linéaire qui prédit les dépenses de santé par ménage à partir de la base de données HISP (Health Insurance Subsidy Program). La base HISP contient des informations concernant les dépenses de santé de ménages américains avant et après la mise en plce du programme d'assurance HISP (les données sont fictives). <br>\n",
    "\n",
    "\n",
    "Pour des raisons de cohérence, vous n'utiliserez que les données décrivant la situation des ménages avant la mise en œuvre du programme HISP (rond=Before).<br>\n",
    "De plus, seules les variables continues ci-dessous seront utilisées pour entraîner votre modèle :\n",
    "- poverty_index \n",
    "- hhsize\n",
    "- age_sp\n",
    "- age_hh\n",
    "- educ_hh\n",
    "- educ_sp\n",
    "- hospital_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et nettoyage des données\n",
    "Comme il s'agit simplement de comparer vos résultats avec ceux obtenus avec les regérsseurs ScikitLearn, il n'est pas nécessaire de séparer les données en jeu de test et jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"datafinalexam.csv\") # importation des données (le fichier datafinalexam.csv et le notebook doivent être dans le même dossier)\n",
    "data=data.loc[data[\"round\"]==\"Before\"] # filtre pour garder la situation avant la mise en place du dispositif\n",
    "features=[\"poverty_index\",\"hhsize\",\"age_sp\",\"age_hh\",\"educ_hh\",\"educ_sp\",\"hospital_distance\"] # liste des variables à utiliser\n",
    "label=['health_expenditures'] # label\n",
    "data=data[features+label] # selction des colonnes utiles\n",
    "Train_set_features=data[features] # variables du jeu d'entrainement \n",
    "Train_set_label=data[label] # labeldu jeu d'entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres\n",
    "eta=0.00001\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=1e-05, learning_rate='constant')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install scikit-learn\n",
    "from sklearn.linear_model import SGDRegressor # import de la classe SGDRegressor\n",
    "reg = SGDRegressor(fit_intercept=True,learning_rate=\"constant\",eta0=eta,max_iter=K) # Instancition d'un régresseur\n",
    "reg.fit(Train_set_features,Train_set_label) # entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.84334900e-01  2.54070131e-01 -1.36515921e+00  2.47426159e-02\n",
      "  1.30648998e-01  2.47700245e-01  2.59747413e-01 -1.57155339e-04]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient du modèle linéaire\n",
    "Theta=np.concatenate((reg.intercept_,reg.coef_),axis=None)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.922943184473351\n"
     ]
    }
   ],
   "source": [
    "# Performance du modèle Scikit-Learn (RMSE sur jeu d'entrainement)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "Pred=reg.predict(Train_set_features) # prédiction faites par le modèle\n",
    "MSE=mean_squared_error(Pred,Train_set_label) # MSE entre prédictions et valeurs réelles\n",
    "RMSE=MSE**(1/2) # RMSE\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - Votre approche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données dans un format compatible avec vos fonctions\n",
    "X=Stand_Trans(Train_set_features.values,Train_set_features.shape[0],Train_set_features.shape[1])\n",
    "Y=Train_set_label.values.reshape(Train_set_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de X : (9913, 8)\n",
      "Dimensions de Y : (9913,)\n"
     ]
    }
   ],
   "source": [
    "# Affichez les dimensions de X et de Y\n",
    "print('Dimensions de X :', X.shape)\n",
    "print('Dimensions de Y :', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta \n",
    "Theta=[20,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (8,) and (2,) not aligned: 8 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/g6ksr7hd0mjcyjwkj_mqdmgm0000gn/T/ipykernel_8540/3800371845.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calcul des Theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGrad_Desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stochastic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/7g/g6ksr7hd0mjcyjwkj_mqdmgm0000gn/T/ipykernel_8540/3228150552.py\u001b[0m in \u001b[0;36mGrad_Desc\u001b[0;34m(X, Y, Eta, K, Grad_function)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mGrad_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'stochastic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrad_Stochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#on recalcule la dérivée à chaque fois\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mGrad_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'regular'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrad_Regular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#on recalcule la dérivée à chaque fois\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/7g/g6ksr7hd0mjcyjwkj_mqdmgm0000gn/T/ipykernel_8540/512987321.py\u001b[0m in \u001b[0;36mGrad_Stochastic\u001b[0;34m(X, Y, Theta)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#Xi = Xi.reshape(2, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Theta = np.reshape(Theta, (2, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#impossible avec X.shape(2,) et Y(100,) donc j'ai mis Y[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Grad ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (8,) and (2,) not aligned: 8 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Calcul des Theta\n",
    "Theta=Grad_Desc(X,Y,eta,K,'stochastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=\n",
    "MSE=\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=X.dot(Theta)\n",
    "MSE=mean_squared_error(Pred,Y)\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des résultats\n",
    "Comparer vos résultats à ceux obtenus avec ***SGDRegressor***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
