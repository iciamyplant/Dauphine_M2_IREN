{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n",
    "Dans ce notebook, vous allez programmer l'algorithme de descente de gradient dans sa version ordinaire, stochastique et mini-batch.\n",
    "Dans un premier temps, vous utiliserez un jeu de données simple pour tester votre code dans le cas d'une régressison linéaire. Dans un second temps, vous implémenterz vos algorithmes sur un jeu de donnée plus conséquent et comparerez vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un jeu de données simple pour tester votre code\n",
    "Un je de donnée simple constitué de 100 instance est créé artificiellement à l'aide de la relation linéaire $\\ y_{i} = 2+5\\times x_{i}  $<br>\n",
    "Ce jeu de donnée ne comporte qu'une seule variables $\\ x  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un vecteur X de taille 100 \n",
    "X=np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez X ainsi que ses dimensions\n",
    "print('X =', X) #on affiche X\n",
    "print('Dimension =', X.shape) #montre que X est un tableau à une dimension 100x1\n",
    "print('Taille =', len(X)) #donne la taille de X\n",
    "print('Type =', type(X)) #affichons aussi le type de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clcul de Y\n",
    "Y=2+5*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = [  2   7  12  17  22  27  32  37  42  47  52  57  62  67  72  77  82  87\n",
      "  92  97 102 107 112 117 122 127 132 137 142 147 152 157 162 167 172 177\n",
      " 182 187 192 197 202 207 212 217 222 227 232 237 242 247 252 257 262 267\n",
      " 272 277 282 287 292 297 302 307 312 317 322 327 332 337 342 347 352 357\n",
      " 362 367 372 377 382 387 392 397 402 407 412 417 422 427 432 437 442 447\n",
      " 452 457 462 467 472 477 482 487 492 497]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de Y ainsi que ses dimensions\n",
    "print('Y =', Y)\n",
    "print('Dimension =', Y.shape)\n",
    "print('Taille =', len(Y))\n",
    "print('Type =', type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre jeu de données avec les valeurs $\\ x_{i} $ et leur label $\\ y_{i} $ correspondant, il faut transformer le vecteur $\\ X $ poour la faire correpondre à une matrice de forme générale :\n",
    "\n",
    "$$\\begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1j} & ... & x_{1n}\\\\ 1 & x_{21} & x_{22} & ... & x_{2j} & ... & x_{2n} \\\\...&...&...&...&...&...&... \\\\1 & x_{i1} & x_{i2} & ... & x_{ij} & ... & x_{in}\\\\...&...&...&...&...&...&...\\\\1 & x_{m1} & x_{m2} & ... & x_{mj} & ... & x_{mn} \\end{bmatrix}$$ <br>\n",
    "Nous réaliserons cette transformation à l'aide d'une fonction ***Stand_Trans***  qui prend en entrée la matrice $\\ X $ à transformer, le nombre de d'instances $\\ m $ et le nombre de variables $\\ n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stand_Trans(X,m,n):\n",
    "    X=X.reshape(m,n) # Transorme la matrice en entrée en une matrice de dimension m*n\n",
    "    Ones=np.ones((m)).reshape(m,1) #crée une matrice de dimension m*1 remplie de 1\n",
    "    X=np.hstack([Ones,X]) # Concatenation horizontale des matrice X et Ones pour donner lieu à une nouvelle matrice X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez la fonction Stand_Trans au vecteur X\n",
    "X=Stand_Trans(X,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  3.]\n",
      " [ 1.  4.]\n",
      " [ 1.  5.]\n",
      " [ 1.  6.]\n",
      " [ 1.  7.]\n",
      " [ 1.  8.]\n",
      " [ 1.  9.]\n",
      " [ 1. 10.]\n",
      " [ 1. 11.]\n",
      " [ 1. 12.]\n",
      " [ 1. 13.]\n",
      " [ 1. 14.]\n",
      " [ 1. 15.]\n",
      " [ 1. 16.]\n",
      " [ 1. 17.]\n",
      " [ 1. 18.]\n",
      " [ 1. 19.]\n",
      " [ 1. 20.]\n",
      " [ 1. 21.]\n",
      " [ 1. 22.]\n",
      " [ 1. 23.]\n",
      " [ 1. 24.]\n",
      " [ 1. 25.]\n",
      " [ 1. 26.]\n",
      " [ 1. 27.]\n",
      " [ 1. 28.]\n",
      " [ 1. 29.]\n",
      " [ 1. 30.]\n",
      " [ 1. 31.]\n",
      " [ 1. 32.]\n",
      " [ 1. 33.]\n",
      " [ 1. 34.]\n",
      " [ 1. 35.]\n",
      " [ 1. 36.]\n",
      " [ 1. 37.]\n",
      " [ 1. 38.]\n",
      " [ 1. 39.]\n",
      " [ 1. 40.]\n",
      " [ 1. 41.]\n",
      " [ 1. 42.]\n",
      " [ 1. 43.]\n",
      " [ 1. 44.]\n",
      " [ 1. 45.]\n",
      " [ 1. 46.]\n",
      " [ 1. 47.]\n",
      " [ 1. 48.]\n",
      " [ 1. 49.]\n",
      " [ 1. 50.]\n",
      " [ 1. 51.]\n",
      " [ 1. 52.]\n",
      " [ 1. 53.]\n",
      " [ 1. 54.]\n",
      " [ 1. 55.]\n",
      " [ 1. 56.]\n",
      " [ 1. 57.]\n",
      " [ 1. 58.]\n",
      " [ 1. 59.]\n",
      " [ 1. 60.]\n",
      " [ 1. 61.]\n",
      " [ 1. 62.]\n",
      " [ 1. 63.]\n",
      " [ 1. 64.]\n",
      " [ 1. 65.]\n",
      " [ 1. 66.]\n",
      " [ 1. 67.]\n",
      " [ 1. 68.]\n",
      " [ 1. 69.]\n",
      " [ 1. 70.]\n",
      " [ 1. 71.]\n",
      " [ 1. 72.]\n",
      " [ 1. 73.]\n",
      " [ 1. 74.]\n",
      " [ 1. 75.]\n",
      " [ 1. 76.]\n",
      " [ 1. 77.]\n",
      " [ 1. 78.]\n",
      " [ 1. 79.]\n",
      " [ 1. 80.]\n",
      " [ 1. 81.]\n",
      " [ 1. 82.]\n",
      " [ 1. 83.]\n",
      " [ 1. 84.]\n",
      " [ 1. 85.]\n",
      " [ 1. 86.]\n",
      " [ 1. 87.]\n",
      " [ 1. 88.]\n",
      " [ 1. 89.]\n",
      " [ 1. 90.]\n",
      " [ 1. 91.]\n",
      " [ 1. 92.]\n",
      " [ 1. 93.]\n",
      " [ 1. 94.]\n",
      " [ 1. 95.]\n",
      " [ 1. 96.]\n",
      " [ 1. 97.]\n",
      " [ 1. 98.]\n",
      " [ 1. 99.]]\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de la nouvelle matrice X\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez qu'à ce stade votre matrice $\\ X $ est de dimension $\\ 100 \\times 2 $ et que la première colonne colonne ne contient que des $\\ 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation normale\n",
    "Dans le cas de la régression linéaire ($\\ \\hat{Y}= X \\cdot \\theta $ ), l'équation normale donne une solution exacte à la problématique de minimisation de la fonction ***Loss*** lorque celle-ci correspond à une MSE (Mean Squared Error).<br>\n",
    "\\begin{equation}\n",
    "\\theta=(X^T \\times X)^{-1} \\times X^T \\times Y\n",
    "\\end{equation}<br>\n",
    "Pour calculer les $\\ \\theta_{j} $, on utilise les méthodes de calcul matriciel de la bibliothèque ***numpy***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 5.]\n"
     ]
    }
   ],
   "source": [
    "Theta=np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction = 507.00000000000006\n"
     ]
    }
   ],
   "source": [
    "X1 = [1 , 101]\n",
    "prediction = np.dot(X1, Theta)\n",
    "print('prediction =', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des paramètres $\\ \\theta_{j} $ obtenues avec l'équation normale sont optimales. Néanmoins, le calcul peut être long lorsque le nombre de variables est grand. La descente de gradient est une alternative à l'équation normale dans le cas linéaire lorsque le nombre de variables est grand. \n",
    "Par ailleurs, la descente de gradient s'applique à des modèles non-linéaires pour lesuqles il n'existe pas de solution exacte à la problamatique de minimisation de la ***loss***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procédure générique de descente de gradient\n",
    "La descente de gradient est une approche ittérative qui conssiste à mettre à jour les pramètres $\\ \\theta_{j} $ en effectuant un déplacement dans le sens opposé au gradient.<br>\n",
    "De façon générique, la mise à jour des paramètres $\\ \\theta_{j} $ s’effectue comme suit :<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^k=\\theta^{k-1}- \\eta \\times \\nabla_{\\theta}MSE \n",
    "\\end{equation}<br>\n",
    "\n",
    "Avec :<br>\n",
    "- $\\ \\theta $ : le vecteur des paramètres $\\ \\theta_{j} $\n",
    "- $\\ k $ : le numéro d'itération en cours\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ \\nabla_{\\theta}MSE$ : le vecteur gradient <br>\n",
    "\n",
    "*Remarque: l'initialisation de $\\ \\theta $ est arbitraire.*<br>\n",
    "\n",
    "Il existe de nombreuse variantes de la descente de gradient qui se différentient par la façon de calculer $\\ \\nabla_{\\theta}MSE$. <br>\n",
    "Nous pouvons donc écrire une fonction générique de descente de gradient dont un des arguments est lui-même une fonction qui précise la façons de calculer $\\ \\nabla_{\\theta}MSE$.<br>\n",
    "Ecrvivez la fonction ***Grad_Desc*** qui renvoie en sortie $\\ \\theta $ et qui prend comme arguments:<br>\n",
    "- $\\ X $: la matrice des données d'apprentissage\n",
    "- $\\ Y $: le vecteur des labels\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ K $ : le nombre total d'itérations\n",
    "- $\\ Grad $ : la fonction de calcul de $\\ \\nabla_{\\theta}MSE$ <br>\n",
    "\n",
    "*Remarque : dans un premier temps, faites comme si la fonction ***Grad*** était connue. Vous en préciserez le comportement dans un second temps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eta = learning_rate\n",
    "#K = nb_iterations\n",
    "#Grad = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc(X,Y,Eta,K,Grad_function): #algorithme de gradient\n",
    "    Theta = [7, 8] #random initialisation\n",
    "    for i in range(0, K):\n",
    "        if Grad_function == 'stochastic':\n",
    "            Grad = Grad_Stochastic(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        elif Grad_function == 'regular':\n",
    "            Grad = Grad_Regular(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        else :\n",
    "            print('Syntax Error :\"', Grad_function, '\"')\n",
    "            return\n",
    "        Theta = Theta - Eta * Grad #on fait un petit pas\n",
    "        print('i =', i, 'Theta =', Theta)\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient ordinaire\n",
    "Les composantes du vecteur gradient correpondent aux dérivées partielles de la fonction ***Loss*** par rapport à chaque paramètre $\\ \\theta_{j} $. Le vecteur gradient s'écrit comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}MSE=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{0}}\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{1}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{n}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Dans le cas de la descente de gradient ordinaire, la totalité du jeu d'entrainement est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous. <br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i=1}^{m} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= \\dfrac{2}{m} \\times X^T\\cdot(X\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Regular*** qui calcule le gradient ordinaire en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Regular(X,Y,Theta): #calcule le vecteur de gradient\n",
    "    m = len(Y)\n",
    "    Grad = 2/m * X.transpose().dot(X.dot(Theta) - Y)\n",
    "    #print('TYPE==================', type(Theta))\n",
    "    #print('TYPE==================', type(X))\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Regular***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [  307. 20196.]\n",
      "i = 0 Theta = [6.9693 5.9804]\n",
      "Grad = [ 106.9982 6930.2475]\n",
      "i = 1 Theta = [6.95860018 5.28737525]\n",
      "Grad = [  38.36735011 2378.09468457]\n",
      "i = 2 Theta = [6.95476344 5.04956578]\n",
      "Grad = [ 14.81653926 816.02006845]\n",
      "i = 3 Theta = [6.95328179 4.96796377]\n",
      "Grad = [  6.73497728 279.99300576]\n",
      "i = 4 Theta = [6.95260829 4.93996447]\n",
      "Grad = [ 3.96169952 96.0549226 ]\n",
      "i = 5 Theta = [6.95221212 4.93035898]\n",
      "Grad = [ 3.00996345 32.9364341 ]\n",
      "i = 6 Theta = [6.95191113 4.92706534]\n",
      "Grad = [ 2.68329076 11.27727919]\n",
      "i = 7 Theta = [6.9516428  4.92593761]\n",
      "Grad = [2.57110904 3.84492537]\n",
      "i = 8 Theta = [6.95138569 4.92555312]\n",
      "Grad = [2.53253006 1.2945089 ]\n",
      "i = 9 Theta = [6.95113243 4.92542367]\n",
      "Grad = [2.51920791 0.41933286]\n",
      "i = 10 Theta = [6.95088051 4.92538173]\n",
      "Grad = [2.51455267 0.11901681]\n",
      "i = 11 Theta = [6.95062906 4.92536983]\n",
      "Grad = [2.5128715 0.0159644]\n",
      "i = 12 Theta = [6.95037777 4.92536824]\n",
      "Grad = [ 2.51221088 -0.01939685]\n",
      "i = 13 Theta = [6.95012655 4.92537018]\n",
      "Grad = [ 2.51190046 -0.03152983]\n",
      "i = 14 Theta = [6.94987536 4.92537333]\n",
      "Grad = [ 2.51171023 -0.035692  ]\n",
      "i = 15 Theta = [6.94962419 4.9253769 ]\n",
      "Grad = [ 2.51156124 -0.037119  ]\n",
      "i = 16 Theta = [6.94937303 4.92538061]\n",
      "Grad = [ 2.5114264  -0.03760741]\n",
      "i = 17 Theta = [6.94912189 4.92538437]\n",
      "Grad = [ 2.51129643 -0.03777374]\n",
      "i = 18 Theta = [6.94887076 4.92538815]\n",
      "Grad = [ 2.51116813 -0.03782956]\n",
      "i = 19 Theta = [6.94861964 4.92539193]\n",
      "Grad = [ 2.51104041 -0.03784745]\n",
      "i = 20 Theta = [6.94836854 4.92539572]\n",
      "Grad = [ 2.51091289 -0.03785233]\n",
      "i = 21 Theta = [6.94811745 4.9253995 ]\n",
      "Grad = [ 2.51078545 -0.03785274]\n",
      "i = 22 Theta = [6.94786637 4.92540329]\n",
      "Grad = [ 2.51065803 -0.03785162]\n",
      "i = 23 Theta = [6.9476153  4.92540707]\n",
      "Grad = [ 2.51053063 -0.03784998]\n",
      "i = 24 Theta = [6.94736425 4.92541086]\n",
      "Grad = [ 2.51040324 -0.03784815]\n",
      "i = 25 Theta = [6.94711321 4.92541464]\n",
      "Grad = [ 2.51027586 -0.03784626]\n",
      "i = 26 Theta = [6.94686218 4.92541843]\n",
      "Grad = [ 2.51014848 -0.03784435]\n",
      "i = 27 Theta = [6.94661117 4.92542221]\n",
      "Grad = [ 2.51002111 -0.03784244]\n",
      "i = 28 Theta = [6.94636017 4.92542599]\n",
      "Grad = [ 2.50989374 -0.03784052]\n",
      "i = 29 Theta = [6.94610918 4.92542978]\n",
      "Grad = [ 2.50976639 -0.0378386 ]\n",
      "i = 30 Theta = [6.9458582  4.92543356]\n",
      "Grad = [ 2.50963904 -0.03783668]\n",
      "i = 31 Theta = [6.94560724 4.92543735]\n",
      "Grad = [ 2.50951169 -0.03783476]\n",
      "i = 32 Theta = [6.94535628 4.92544113]\n",
      "Grad = [ 2.50938435 -0.03783284]\n",
      "i = 33 Theta = [6.94510535 4.92544491]\n",
      "Grad = [ 2.50925702 -0.03783092]\n",
      "i = 34 Theta = [6.94485442 4.9254487 ]\n",
      "Grad = [ 2.5091297 -0.037829 ]\n",
      "i = 35 Theta = [6.94460351 4.92545248]\n",
      "Grad = [ 2.50900238 -0.03782708]\n",
      "i = 36 Theta = [6.94435261 4.92545626]\n",
      "Grad = [ 2.50887506 -0.03782516]\n",
      "i = 37 Theta = [6.94410172 4.92546004]\n",
      "Grad = [ 2.50874776 -0.03782324]\n",
      "i = 38 Theta = [6.94385085 4.92546383]\n",
      "Grad = [ 2.50862046 -0.03782132]\n",
      "i = 39 Theta = [6.94359998 4.92546761]\n",
      "Grad = [ 2.50849317 -0.0378194 ]\n",
      "i = 40 Theta = [6.94334913 4.92547139]\n",
      "Grad = [ 2.50836588 -0.03781748]\n",
      "i = 41 Theta = [6.9430983  4.92547517]\n",
      "Grad = [ 2.5082386  -0.03781556]\n",
      "i = 42 Theta = [6.94284747 4.92547895]\n",
      "Grad = [ 2.50811133 -0.03781365]\n",
      "i = 43 Theta = [6.94259666 4.92548273]\n",
      "Grad = [ 2.50798406 -0.03781173]\n",
      "i = 44 Theta = [6.94234586 4.92548652]\n",
      "Grad = [ 2.5078568  -0.03780981]\n",
      "i = 45 Theta = [6.94209508 4.9254903 ]\n",
      "Grad = [ 2.50772954 -0.03780789]\n",
      "i = 46 Theta = [6.94184431 4.92549408]\n",
      "Grad = [ 2.5076023  -0.03780597]\n",
      "i = 47 Theta = [6.94159354 4.92549786]\n",
      "Grad = [ 2.50747505 -0.03780405]\n",
      "i = 48 Theta = [6.9413428  4.92550164]\n",
      "Grad = [ 2.50734782 -0.03780213]\n",
      "i = 49 Theta = [6.94109206 4.92550542]\n",
      "Grad = [ 2.50722059 -0.03780022]\n",
      "i = 50 Theta = [6.94084134 4.9255092 ]\n",
      "Grad = [ 2.50709337 -0.0377983 ]\n",
      "i = 51 Theta = [6.94059063 4.92551298]\n",
      "Grad = [ 2.50696615 -0.03779638]\n",
      "i = 52 Theta = [6.94033993 4.92551676]\n",
      "Grad = [ 2.50683894 -0.03779446]\n",
      "i = 53 Theta = [6.94008925 4.92552054]\n",
      "Grad = [ 2.50671174 -0.03779254]\n",
      "i = 54 Theta = [6.93983858 4.92552432]\n",
      "Grad = [ 2.50658455 -0.03779063]\n",
      "i = 55 Theta = [6.93958792 4.9255281 ]\n",
      "Grad = [ 2.50645736 -0.03778871]\n",
      "i = 56 Theta = [6.93933728 4.92553187]\n",
      "Grad = [ 2.50633017 -0.03778679]\n",
      "i = 57 Theta = [6.93908664 4.92553565]\n",
      "Grad = [ 2.506203   -0.03778487]\n",
      "i = 58 Theta = [6.93883602 4.92553943]\n",
      "Grad = [ 2.50607583 -0.03778296]\n",
      "i = 59 Theta = [6.93858541 4.92554321]\n",
      "Grad = [ 2.50594866 -0.03778104]\n",
      "i = 60 Theta = [6.93833482 4.92554699]\n",
      "Grad = [ 2.5058215  -0.03777912]\n",
      "i = 61 Theta = [6.93808424 4.92555077]\n",
      "Grad = [ 2.50569435 -0.03777721]\n",
      "i = 62 Theta = [6.93783367 4.92555454]\n",
      "Grad = [ 2.50556721 -0.03777529]\n",
      "i = 63 Theta = [6.93758311 4.92555832]\n",
      "Grad = [ 2.50544007 -0.03777337]\n",
      "i = 64 Theta = [6.93733257 4.9255621 ]\n",
      "Grad = [ 2.50531294 -0.03777146]\n",
      "i = 65 Theta = [6.93708204 4.92556588]\n",
      "Grad = [ 2.50518581 -0.03776954]\n",
      "i = 66 Theta = [6.93683152 4.92556965]\n",
      "Grad = [ 2.5050587  -0.03776762]\n",
      "i = 67 Theta = [6.93658101 4.92557343]\n",
      "Grad = [ 2.50493158 -0.03776571]\n",
      "i = 68 Theta = [6.93633052 4.92557721]\n",
      "Grad = [ 2.50480448 -0.03776379]\n",
      "i = 69 Theta = [6.93608004 4.92558098]\n",
      "Grad = [ 2.50467738 -0.03776187]\n",
      "i = 70 Theta = [6.93582957 4.92558476]\n",
      "Grad = [ 2.50455029 -0.03775996]\n",
      "i = 71 Theta = [6.93557912 4.92558854]\n",
      "Grad = [ 2.5044232  -0.03775804]\n",
      "i = 72 Theta = [6.93532867 4.92559231]\n",
      "Grad = [ 2.50429612 -0.03775613]\n",
      "i = 73 Theta = [6.93507824 4.92559609]\n",
      "Grad = [ 2.50416904 -0.03775421]\n",
      "i = 74 Theta = [6.93482783 4.92559986]\n",
      "Grad = [ 2.50404198 -0.03775229]\n",
      "i = 75 Theta = [6.93457742 4.92560364]\n",
      "Grad = [ 2.50391492 -0.03775038]\n",
      "i = 76 Theta = [6.93432703 4.92560741]\n",
      "Grad = [ 2.50378786 -0.03774846]\n",
      "i = 77 Theta = [6.93407665 4.92561119]\n",
      "Grad = [ 2.50366082 -0.03774655]\n",
      "i = 78 Theta = [6.93382629 4.92561496]\n",
      "Grad = [ 2.50353377 -0.03774463]\n",
      "i = 79 Theta = [6.93357593 4.92561874]\n",
      "Grad = [ 2.50340674 -0.03774272]\n",
      "i = 80 Theta = [6.93332559 4.92562251]\n",
      "Grad = [ 2.50327971 -0.0377408 ]\n",
      "i = 81 Theta = [6.93307526 4.92562628]\n",
      "Grad = [ 2.50315269 -0.03773889]\n",
      "i = 82 Theta = [6.93282495 4.92563006]\n",
      "Grad = [ 2.50302567 -0.03773697]\n",
      "i = 83 Theta = [6.93257465 4.92563383]\n",
      "Grad = [ 2.50289866 -0.03773506]\n",
      "i = 84 Theta = [6.93232436 4.92563761]\n",
      "Grad = [ 2.50277166 -0.03773314]\n",
      "i = 85 Theta = [6.93207408 4.92564138]\n",
      "Grad = [ 2.50264466 -0.03773123]\n",
      "i = 86 Theta = [6.93182381 4.92564515]\n",
      "Grad = [ 2.50251767 -0.03772931]\n",
      "i = 87 Theta = [6.93157356 4.92564892]\n",
      "Grad = [ 2.50239069 -0.0377274 ]\n",
      "i = 88 Theta = [6.93132332 4.9256527 ]\n",
      "Grad = [ 2.50226371 -0.03772548]\n",
      "i = 89 Theta = [6.9310731  4.92565647]\n",
      "Grad = [ 2.50213674 -0.03772357]\n",
      "i = 90 Theta = [6.93082288 4.92566024]\n",
      "Grad = [ 2.50200978 -0.03772166]\n",
      "i = 91 Theta = [6.93057268 4.92566401]\n",
      "Grad = [ 2.50188282 -0.03771974]\n",
      "i = 92 Theta = [6.93032249 4.92566779]\n",
      "Grad = [ 2.50175587 -0.03771783]\n",
      "i = 93 Theta = [6.93007232 4.92567156]\n",
      "Grad = [ 2.50162893 -0.03771591]\n",
      "i = 94 Theta = [6.92982216 4.92567533]\n",
      "Grad = [ 2.50150199 -0.037714  ]\n",
      "i = 95 Theta = [6.92957201 4.9256791 ]\n",
      "Grad = [ 2.50137506 -0.03771209]\n",
      "i = 96 Theta = [6.92932187 4.92568287]\n",
      "Grad = [ 2.50124813 -0.03771017]\n",
      "i = 97 Theta = [6.92907174 4.92568664]\n",
      "Grad = [ 2.50112121 -0.03770826]\n",
      "i = 98 Theta = [6.92882163 4.92569041]\n",
      "Grad = [ 2.5009943  -0.03770635]\n",
      "i = 99 Theta = [6.92857153 4.92569419]\n"
     ]
    }
   ],
   "source": [
    "Theta = Grad_Desc(X,Y,0.0001,100,'regular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_regular = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_regular = np.dot(X1, Theta)\n",
    "print('prediction_regular =', prediction_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient stochastique\n",
    "Dans le cas de la descente de gradient stochastique, seule une instance $\\ X_{i} $ du jeu d'entrainement, tirée au hasard à chaque itération, est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous.<br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=2 \\times x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Pour une instance $\\ i$ prise au hasard, le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= 2 \\times X_{i}\\cdot(X_{i}\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Stochastic*** qui calcule le gradient stochastique en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # à utiliser pour le tirage aléatoire d'une instance Xi à chaque itération\n",
    "def Grad_Stochastic(X,Y,Theta):\n",
    "    i = random.randrange(0, 99, 1)\n",
    "    #Xi = Xi.reshape(2, 1)\n",
    "    #Theta = np.reshape(Theta, (2, 1))\n",
    "    Grad = 2 * X[i].dot(X[i].dot(Theta) - Y[i]) #impossible avec X.shape(2,) et Y(100,) donc j'ai mis Y[i]\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient stochastique sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Stochastic***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [  88. 1144.]\n",
      "i = 0 Theta = [6.9912 7.8856]\n",
      "Grad = [ 113.864 2049.552]\n",
      "i = 1 Theta = [6.9798136 7.6806448]\n",
      "Grad = [  299.4692656 16171.3403424]\n",
      "i = 2 Theta = [6.94986667 6.06351077]\n",
      "Grad = [14.15377641 28.30755282]\n",
      "i = 3 Theta = [6.9484513  6.06068001]\n",
      "Grad = [16.26098265 48.78294796]\n",
      "i = 4 Theta = [6.9468252  6.05580172]\n",
      "Grad = [  153.48268373 10436.82249349]\n",
      "i = 5 Theta = [6.93147693 5.01211947]\n",
      "Grad = [ 10.25077678 164.0124285 ]\n",
      "i = 6 Theta = [6.93045185 4.99571822]\n",
      "Grad = [  9.54405224 353.12993291]\n",
      "i = 7 Theta = [6.92949745 4.96040523]\n",
      "Grad = [  5.26600159 305.4280925 ]\n",
      "i = 8 Theta = [6.92897085 4.92986242]\n",
      "Grad = [  -3.18764801 -296.45126516]\n",
      "i = 9 Theta = [6.92928961 4.95950755]\n",
      "Grad = [  2.48895286 226.4947102 ]\n",
      "i = 10 Theta = [6.92904072 4.93685808]\n",
      "Grad = [  2.02848291 125.7659404 ]\n",
      "i = 11 Theta = [6.92883787 4.92428148]\n",
      "Grad = [ 1.22576473 69.8685895 ]\n",
      "i = 12 Theta = [6.92871529 4.91729462]\n",
      "Grad = [  5.556751   144.47552603]\n",
      "i = 13 Theta = [6.92815962 4.90284707]\n",
      "Grad = [  6.16450792 117.12565055]\n",
      "i = 14 Theta = [6.92754316 4.89113451]\n",
      "Grad = [  -2.99104199 -176.47147722]\n",
      "i = 15 Theta = [6.92784227 4.90878165]\n",
      "Grad = [  7.11913414 106.78701216]\n",
      "i = 16 Theta = [6.92713036 4.89810295]\n",
      "Grad = [ 1.7024969  68.09987581]\n",
      "i = 17 Theta = [6.92696011 4.89129296]\n",
      "Grad = [ 8.33202172 58.32415203]\n",
      "i = 18 Theta = [6.9261269  4.88546055]\n",
      "Grad = [ -10.53576838 -937.68338572]\n",
      "i = 19 Theta = [6.92718048 4.97922889]\n",
      "Grad = [  6.11556082 550.40047351]\n",
      "i = 20 Theta = [6.92656892 4.92418884]\n",
      "Grad = [  -1.36691372 -101.15161543]\n",
      "i = 21 Theta = [6.92670562 4.934304  ]\n",
      "Grad = [ 0.78736355 54.3280847 ]\n",
      "i = 22 Theta = [6.92662688 4.92887119]\n",
      "Grad = [  -2.52315852 -219.51479108]\n",
      "i = 23 Theta = [6.9268792  4.95082267]\n",
      "Grad = [  6.31299084 227.26767027]\n",
      "i = 24 Theta = [6.9262479  4.92809591]\n",
      "Grad = [ 1.65542906 94.35945665]\n",
      "i = 25 Theta = [6.92608235 4.91865996]\n",
      "Grad = [  -4.78904245 -431.01382076]\n",
      "i = 26 Theta = [6.92656126 4.96176134]\n",
      "Grad = [  2.35834561 231.11786939]\n",
      "i = 27 Theta = [6.92632542 4.93864956]\n",
      "Grad = [ -1.0677283  -95.02781867]\n",
      "i = 28 Theta = [6.9264322  4.94815234]\n",
      "Grad = [  1.66093367 131.21376023]\n",
      "i = 29 Theta = [6.9262661  4.93503096]\n",
      "Grad = [  6.9938945  153.86567896]\n",
      "i = 30 Theta = [6.92556671 4.91964439]\n",
      "Grad = [  5.35121945 149.83414465]\n",
      "i = 31 Theta = [6.92503159 4.90466098]\n",
      "Grad = [  -8.64570692 -838.63357144]\n",
      "i = 32 Theta = [6.92589616 4.98852434]\n",
      "Grad = [ 9.62227904 96.22279044]\n",
      "i = 33 Theta = [6.92493393 4.97890206]\n",
      "Grad = [  9.21692958 138.25394365]\n",
      "i = 34 Theta = [6.92401224 4.96507666]\n",
      "Grad = [  4.53967719 345.01546675]\n",
      "i = 35 Theta = [6.92355827 4.93057512]\n",
      "Grad = [  3.87657651 166.69279012]\n",
      "i = 36 Theta = [6.92317062 4.91390584]\n",
      "Grad = [ -0.65714667 -40.08594674]\n",
      "i = 37 Theta = [6.92323633 4.91791443]\n",
      "Grad = [  7.21973447 115.51575153]\n",
      "i = 38 Theta = [6.92251436 4.90636286]\n",
      "Grad = [ -1.39142852 -83.485711  ]\n",
      "i = 39 Theta = [6.9226535  4.91471143]\n",
      "Grad = [  5.75145552 138.03493248]\n",
      "i = 40 Theta = [6.92207835 4.90090793]\n",
      "Grad = [  -8.38878339 -771.76807161]\n",
      "i = 41 Theta = [6.92291723 4.97808474]\n",
      "Grad = [  5.90108792 531.09791294]\n",
      "i = 42 Theta = [6.92232712 4.92497495]\n",
      "Grad = [ 0.99169836 58.51020342]\n",
      "i = 43 Theta = [6.92222795 4.91912393]\n",
      "Grad = [ -0.1841768  -11.41896149]\n",
      "i = 44 Theta = [6.92224637 4.92026583]\n",
      "Grad = [ 1.0737336  59.05534781]\n",
      "i = 45 Theta = [6.922139   4.91436029]\n",
      "Grad = [ 1.62286595 77.89756549]\n",
      "i = 46 Theta = [6.92197671 4.90657053]\n",
      "Grad = [ 8.34908198 66.79265582]\n",
      "i = 47 Theta = [6.9211418  4.89989127]\n",
      "Grad = [ 8.44076137 59.08532961]\n",
      "i = 48 Theta = [6.92029773 4.89398274]\n",
      "Grad = [  -5.21385602 -370.1837777 ]\n",
      "i = 49 Theta = [6.92081911 4.93100111]\n",
      "Grad = [  2.52775629 133.9710835 ]\n",
      "i = 50 Theta = [6.92056634 4.91760401]\n",
      "Grad = [  6.54529289 130.90585788]\n",
      "i = 51 Theta = [6.91991181 4.90451342]\n",
      "Grad = [ 2.00992403 82.40688528]\n",
      "i = 52 Theta = [6.91971082 4.89627273]\n",
      "Grad = [  -3.0227597  -187.41110163]\n",
      "i = 53 Theta = [6.92001309 4.91501384]\n",
      "Grad = [9.84002618 0.        ]\n",
      "i = 54 Theta = [6.91902909 4.91501384]\n",
      "Grad = [  2.52924853 108.7576869 ]\n",
      "i = 55 Theta = [6.91877616 4.90413807]\n",
      "Grad = [  -7.22587074 -643.10249598]\n",
      "i = 56 Theta = [6.91949875 4.96844832]\n",
      "Grad = [  4.85383242 383.4527609 ]\n",
      "i = 57 Theta = [6.91901337 4.93010305]\n",
      "Grad = [  4.80544606 172.99605812]\n",
      "i = 58 Theta = [6.91853282 4.91280344]\n",
      "Grad = [  -6.73028069 -639.37666544]\n",
      "i = 59 Theta = [6.91920585 4.97674111]\n",
      "Grad = [  8.86153819 186.09230203]\n",
      "i = 60 Theta = [6.9183197  4.95813188]\n",
      "Grad = [ 9.16674942 73.33399538]\n",
      "i = 61 Theta = [6.91740302 4.95079848]\n",
      "Grad = [ 0.58491975 54.98245632]\n",
      "i = 62 Theta = [6.91734453 4.94530023]\n",
      "Grad = [  2.28612101 157.74234968]\n",
      "i = 63 Theta = [6.91711592 4.929526  ]\n",
      "Grad = [9.69328383 9.69328383]\n",
      "i = 64 Theta = [6.91614659 4.92855667]\n",
      "Grad = [ 1.54486669 89.60226784]\n",
      "i = 65 Theta = [6.9159921  4.91959644]\n",
      "Grad = [  2.2740497  106.88033573]\n",
      "i = 66 Theta = [6.9157647  4.90890841]\n",
      "Grad = [ 0.72237018 36.11850887]\n",
      "i = 67 Theta = [6.91569246 4.90529656]\n",
      "Grad = [ 8.31612983 66.52903867]\n",
      "i = 68 Theta = [6.91486085 4.89864365]\n",
      "Grad = [  -5.17101765 -382.65530622]\n",
      "i = 69 Theta = [6.91537795 4.93690918]\n",
      "Grad = [  4.90967223 191.47721685]\n",
      "i = 70 Theta = [6.91488698 4.91776146]\n",
      "Grad = [  -6.28897948 -616.31998949]\n",
      "i = 71 Theta = [6.91551588 4.97939346]\n",
      "Grad = [  6.24549396 543.35797493]\n",
      "i = 72 Theta = [6.91489133 4.92505766]\n",
      "Grad = [ -0.21249044 -14.23685918]\n",
      "i = 73 Theta = [6.91491258 4.92648135]\n",
      "Grad = [  -2.66834545 -226.80936304]\n",
      "i = 74 Theta = [6.91517942 4.94916229]\n",
      "Grad = [ -0.13383318 -13.11565159]\n",
      "i = 75 Theta = [6.9151928  4.95047385]\n",
      "Grad = [  5.57113677 239.55888102]\n",
      "i = 76 Theta = [6.91463568 4.92651796]\n",
      "Grad = [  3.21588801 144.71496066]\n",
      "i = 77 Theta = [6.9143141  4.91204647]\n",
      "Grad = [  6.13457979 128.82617563]\n",
      "i = 78 Theta = [6.91370064 4.89916385]\n",
      "Grad = [ -1.06290302 -57.39676318]\n",
      "i = 79 Theta = [6.91380693 4.90490353]\n",
      "Grad = [ 1.07873819 49.62195695]\n",
      "i = 80 Theta = [6.91369905 4.89994133]\n",
      "Grad = [0.02164842 1.06077264]\n",
      "i = 81 Theta = [6.91369689 4.89983525]\n",
      "Grad = [  -8.80324926 -818.70218161]\n",
      "i = 82 Theta = [6.91457721 4.98170547]\n",
      "Grad = [  7.48745467 479.19709873]\n",
      "i = 83 Theta = [6.91382847 4.93378576]\n",
      "Grad = [ 1.08737736 71.76690549]\n",
      "i = 84 Theta = [6.91371973 4.92660907]\n",
      "Grad = [  -3.82327348 -355.56443381]\n",
      "i = 85 Theta = [6.91410206 4.96216551]\n",
      "Grad = [  8.69316952 130.39754287]\n",
      "i = 86 Theta = [6.91323274 4.94912576]\n",
      "Grad = [  7.58799889 166.93597563]\n",
      "i = 87 Theta = [6.91247394 4.93243216]\n",
      "Grad = [ -0.71563489 -55.81952124]\n",
      "i = 88 Theta = [6.91254551 4.93801411]\n",
      "Grad = [  5.73402252 189.22274331]\n",
      "i = 89 Theta = [6.9119721  4.91909184]\n",
      "Grad = [  3.67492401 139.64711236]\n",
      "i = 90 Theta = [6.91160461 4.90512713]\n",
      "Grad = [  -1.75128113 -106.82814884]\n",
      "i = 91 Theta = [6.91177974 4.91580994]\n",
      "Grad = [  -3.4784695  -274.79909071]\n",
      "i = 92 Theta = [6.91212759 4.94328985]\n",
      "Grad = [ 9.25715369 46.28576847]\n",
      "i = 93 Theta = [6.91120187 4.93866128]\n",
      "Grad = [ 1.35765974 93.67852235]\n",
      "i = 94 Theta = [6.9110661  4.92929342]\n",
      "Grad = [ 0.63027722 40.96801921]\n",
      "i = 95 Theta = [6.91100308 4.92519662]\n",
      "Grad = [  -2.89456824 -246.03830018]\n",
      "i = 96 Theta = [6.91129253 4.94980045]\n",
      "Grad = [  1.38906088 116.68111365]\n",
      "i = 97 Theta = [6.91115363 4.93813234]\n",
      "Grad = [  3.75927656 184.20455161]\n",
      "i = 98 Theta = [6.9107777  4.91971188]\n",
      "Grad = [  4.20138733 147.04855656]\n",
      "i = 99 Theta = [6.91035756 4.90500703]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.91035756, 4.90500703])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grad_Desc(X,Y,0.0001,100,\"stochastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_stochastic = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_stochastic = np.dot(X1, Theta)\n",
    "print('prediction_stochastic =', prediction_stochastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la descente de gradient mini-batch, un sous-ensemble du jeu d'entrainement, constitué de $\\ q $ exemples tirés aléatoirement, est utilisé à chaque itération pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$. Le nombre $\\ q $  est un hyperparamètre fixé par l'utilisateur avec $\\ q \\le m$.<br>\n",
    "\n",
    "La formule pour le calcul $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ devient:\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i\\in \\mathscr{E}} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Avec $\\mathscr{E} $ l'ensemble des $\\ q $ exemples tirés aléatoirement (cet ensemble est réactualisé à cahque itération).\n",
    "\n",
    "Ecrivez la fonction ***Grad_Batch*** qui calcule le gradient mini_batch en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Already_choose(j_history, j): #renvoie 1 si j a déjà été tiré, 0 sinon\n",
    "    count = 0\n",
    "    for i in j_history:\n",
    "        if (j_history[count] == j):\n",
    "            return (1)\n",
    "        count+=1\n",
    "    return (0)\n",
    "\n",
    "def examples(q): #retourne une liste des exemples tirés, jamais 2 fois le même exemple\n",
    "    j_history = np.zeros(q) #créer un tableau de 0 de taille q pour mettre l'historique des j tirés\n",
    "    i = 0\n",
    "    while i < q: #on parcours le nombre d'exemples demandé\n",
    "        j = random.randrange(0, 99, 1) #on tire au hasard un exemple\n",
    "        if (Already_choose(j_history, j) == 0): #s'il a pas déjà été tiré\n",
    "            j_history[i] = j #on le rentre dans notre liste d'exemples\n",
    "            i+=1 #on passe au suivant\n",
    "    return(j_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Batch(X,Y,Theta,q):\n",
    "    j_history = examples(q)\n",
    "    count, sum = 0, 0\n",
    "    i = 0\n",
    "    m = len(Y) \n",
    "    while count < q:\n",
    "        i = int(j_history[count])\n",
    "        #sum = sum + x[i][j].dot(X[i].dot(Theta) - Y[i]) #???? c'est quoi x[i][j]\n",
    "        sum = sum + X[i].dot(X[i].dot(Theta) - Y[i]) #???? \n",
    "        count+=1\n",
    "    Grad = 2/m * sum\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Batch***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$\n",
    "- $\\ q=10$\n",
    "\n",
    "*Remarque : pensez à adapter la fonction ***Grad_Desc*** en ***Grad_Desc_Batch***  pour tenir compte de l'hyperparamètre $\\ q $ spécifique à l'approche mini-batch. Dans la nouvelle fonction ***Grad_Desc_Batch***, le paramètre ***Grad*** prend par défaut la valeur ***Grad_Batch***.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc_Batch(X,Y,Eta,K,q,Grad=Grad_Batch):\n",
    "    Theta = [7, 12] #random initialisation\n",
    "    for i in range(0, K):\n",
    "        Grad = Grad_Batch(X,Y,Theta,q)\n",
    "        Theta = Theta - Eta * Grad #on fait un petit pas\n",
    "        print('i =', i, 'Theta =', Theta)\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 Theta = [ 6.995518 11.767088]\n",
      "i = 1 Theta = [ 6.98833972 11.30956406]\n",
      "i = 2 Theta = [ 6.98305349 10.96296789]\n",
      "i = 3 Theta = [ 6.97576249 10.46023118]\n",
      "i = 4 Theta = [ 6.96959119 10.02309961]\n",
      "i = 5 Theta = [6.96481027 9.74683882]\n",
      "i = 6 Theta = [6.96023946 9.4975004 ]\n",
      "i = 7 Theta = [6.95504009 9.14645847]\n",
      "i = 8 Theta = [6.95119259 8.89279354]\n",
      "i = 9 Theta = [6.94704506 8.62478731]\n",
      "i = 10 Theta = [6.94376355 8.44143006]\n",
      "i = 11 Theta = [6.94091153 8.28799309]\n",
      "i = 12 Theta = [6.93693946 8.00815793]\n",
      "i = 13 Theta = [6.93364606 7.79500907]\n",
      "i = 14 Theta = [6.93057908 7.59246985]\n",
      "i = 15 Theta = [6.92778949 7.39643724]\n",
      "i = 16 Theta = [6.92564917 7.26871681]\n",
      "i = 17 Theta = [6.92381736 7.17927779]\n",
      "i = 18 Theta = [6.92145679 7.013942  ]\n",
      "i = 19 Theta = [6.9188651 6.8255029]\n",
      "i = 20 Theta = [6.91713472 6.72250272]\n",
      "i = 21 Theta = [6.91499694 6.56984782]\n",
      "i = 22 Theta = [6.91316239 6.44164659]\n",
      "i = 23 Theta = [6.9120146  6.38529401]\n",
      "i = 24 Theta = [6.91042025 6.26931512]\n",
      "i = 25 Theta = [6.90908065 6.18397469]\n",
      "i = 26 Theta = [6.9079382  6.11502117]\n",
      "i = 27 Theta = [6.90692795 6.06108698]\n",
      "i = 28 Theta = [6.9058239  5.99334723]\n",
      "i = 29 Theta = [6.90479999 5.92086184]\n",
      "i = 30 Theta = [6.90380497 5.85544212]\n",
      "i = 31 Theta = [6.90278302 5.77795354]\n",
      "i = 32 Theta = [6.90196147 5.72667785]\n",
      "i = 33 Theta = [6.90105682 5.66399044]\n",
      "i = 34 Theta = [6.90031605 5.62244256]\n",
      "i = 35 Theta = [6.89977736 5.59364719]\n",
      "i = 36 Theta = [6.89910946 5.55330083]\n",
      "i = 37 Theta = [6.89838846 5.50615518]\n",
      "i = 38 Theta = [6.89772765 5.45906982]\n",
      "i = 39 Theta = [6.8971541 5.4238606]\n",
      "i = 40 Theta = [6.8966128  5.38925867]\n",
      "i = 41 Theta = [6.89609991 5.35286471]\n",
      "i = 42 Theta = [6.89562372 5.319003  ]\n",
      "i = 43 Theta = [6.89524636 5.29887529]\n",
      "i = 44 Theta = [6.89488246 5.27746649]\n",
      "i = 45 Theta = [6.89447935 5.25001466]\n",
      "i = 46 Theta = [6.89420245 5.23808272]\n",
      "i = 47 Theta = [6.89381315 5.21026574]\n",
      "i = 48 Theta = [6.89347515 5.18813708]\n",
      "i = 49 Theta = [6.89323467 5.17663482]\n",
      "i = 50 Theta = [6.89290966 5.15305416]\n",
      "i = 51 Theta = [6.89266303 5.13914622]\n",
      "i = 52 Theta = [6.89242742 5.12531225]\n",
      "i = 53 Theta = [6.89222932 5.11638833]\n",
      "i = 54 Theta = [6.8919953  5.10085265]\n",
      "i = 55 Theta = [6.89180528 5.09106058]\n",
      "i = 56 Theta = [6.89159326 5.07634635]\n",
      "i = 57 Theta = [6.89141969 5.06615117]\n",
      "i = 58 Theta = [6.89124394 5.05477192]\n",
      "i = 59 Theta = [6.89109024 5.04600293]\n",
      "i = 60 Theta = [6.89095893 5.04083837]\n",
      "i = 61 Theta = [6.89082534 5.03448534]\n",
      "i = 62 Theta = [6.89068635 5.02559768]\n",
      "i = 63 Theta = [6.89056381 5.01931596]\n",
      "i = 64 Theta = [6.89045116 5.01462797]\n",
      "i = 65 Theta = [6.89034062 5.00966965]\n",
      "i = 66 Theta = [6.89023063 5.00266238]\n",
      "i = 67 Theta = [6.8901303  4.99785326]\n",
      "i = 68 Theta = [6.89003474 4.99288501]\n",
      "i = 69 Theta = [6.88994374 4.98866194]\n",
      "i = 70 Theta = [6.8898578  4.98431698]\n",
      "i = 71 Theta = [6.88977265 4.98099125]\n",
      "i = 72 Theta = [6.88969485 4.97716748]\n",
      "i = 73 Theta = [6.88962815 4.97280097]\n",
      "i = 74 Theta = [6.88955685 4.96984889]\n",
      "i = 75 Theta = [6.88948258 4.96742211]\n",
      "i = 76 Theta = [6.88941971 4.96464686]\n",
      "i = 77 Theta = [6.88935254 4.96207302]\n",
      "i = 78 Theta = [6.88928555 4.95985039]\n",
      "i = 79 Theta = [6.88922446 4.95761237]\n",
      "i = 80 Theta = [6.88918788 4.95513427]\n",
      "i = 81 Theta = [6.8891434  4.95310343]\n",
      "i = 82 Theta = [6.88909383 4.95118325]\n",
      "i = 83 Theta = [6.88903344 4.94941695]\n",
      "i = 84 Theta = [6.8889914 4.9478   ]\n",
      "i = 85 Theta = [6.88894906 4.9462196 ]\n",
      "i = 86 Theta = [6.8889127  4.94478752]\n",
      "i = 87 Theta = [6.88886196 4.94349034]\n",
      "i = 88 Theta = [6.88882295 4.94228091]\n",
      "i = 89 Theta = [6.8887777  4.94102267]\n",
      "i = 90 Theta = [6.88873701 4.93988607]\n",
      "i = 91 Theta = [6.88869971 4.93902605]\n",
      "i = 92 Theta = [6.88864962 4.93784338]\n",
      "i = 93 Theta = [6.88861127 4.93675502]\n",
      "i = 94 Theta = [6.88856789 4.93524632]\n",
      "i = 95 Theta = [6.88850936 4.93369379]\n",
      "i = 96 Theta = [6.88846689 4.93288819]\n",
      "i = 97 Theta = [6.88845422 4.93256781]\n",
      "i = 98 Theta = [6.88843804 4.93340719]\n",
      "i = 99 Theta = [6.88840247 4.93255607]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.88840247, 4.93255607])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grad_Desc_Batch(X,Y,0.0001,100,10,Grad=Grad_Batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_stochastic = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_batch = np.dot(X1, Theta)\n",
    "print('prediction_stochastic =', prediction_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests sur jeu de donnée plus conséquent\n",
    "Vous allez maintenant tester votre code sur un jeu de données plus conséquent et comparer vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***.<br>\n",
    "L'objectif est d'entrainer un régresseur linéaire qui prédit les dépenses de santé par ménage à partir de la base de données HISP (Health Insurance Subsidy Program). La base HISP contient des informations concernant les dépenses de santé de ménages américains avant et après la mise en plce du programme d'assurance HISP (les données sont fictives). <br>\n",
    "\n",
    "\n",
    "Pour des raisons de cohérence, vous n'utiliserez que les données décrivant la situation des ménages avant la mise en œuvre du programme HISP (rond=Before).<br>\n",
    "De plus, seules les variables continues ci-dessous seront utilisées pour entraîner votre modèle :\n",
    "- poverty_index \n",
    "- hhsize\n",
    "- age_sp\n",
    "- age_hh\n",
    "- educ_hh\n",
    "- educ_sp\n",
    "- hospital_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et nettoyage des données\n",
    "Comme il s'agit simplement de comparer vos résultats avec ceux obtenus avec les regérsseurs ScikitLearn, il n'est pas nécessaire de séparer les données en jeu de test et jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"datafinalexam.csv\") # importation des données (le fichier datafinalexam.csv et le notebook doivent être dans le même dossier)\n",
    "data=data.loc[data[\"round\"]==\"Before\"] # filtre pour garder la situation avant la mise en place du dispositif\n",
    "features=[\"poverty_index\",\"hhsize\",\"age_sp\",\"age_hh\",\"educ_hh\",\"educ_sp\",\"hospital_distance\"] # liste des variables à utiliser\n",
    "label=['health_expenditures'] # label\n",
    "data=data[features+label] # selction des colonnes utiles\n",
    "Train_set_features=data[features] # variables du jeu d'entrainement \n",
    "Train_set_label=data[label] # labeldu jeu d'entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres\n",
    "eta=0.00001\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/g6ksr7hd0mjcyjwkj_mqdmgm0000gn/T/ipykernel_5476/1176880509.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m####pip install -U scikit-learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDRegressor\u001b[0m \u001b[0;31m# import de la classe SGDRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Instancition d'un régresseur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_set_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrain_set_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# entrainement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "####pip install -U scikit-learn\n",
    "from sklearn.linear_model import SGDRegressor # import de la classe SGDRegressor\n",
    "reg = SGDRegressor(fit_intercept=True,learning_rate=\"constant\",eta0=eta,max_iter=K) # Instancition d'un régresseur\n",
    "reg.fit(Train_set_features,Train_set_label) # entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37200136  0.26348105 -1.34086898  0.02423312  0.12008257  0.25357157\n",
      "  0.26443865  0.00662281]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient du modèle linéaire\n",
    "Theta=np.concatenate((reg.intercept_,reg.coef_),axis=None)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.941030553940154\n"
     ]
    }
   ],
   "source": [
    "# Performance du modèle Scikit-Learn (RMSE sur jeu d'entrainement)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "Pred=reg.predict(Train_set_features) # prédiction faites par le modèle\n",
    "MSE=mean_squared_error(Pred,Train_set_label) # MSE entre prédictions et valeurs réelles\n",
    "RMSE=MSE**(1/2) # RMSE\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - Votre approche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données dans un format compatible avec vos fonctions\n",
    "X=Stand_Trans(Train_set_features.values,Train_set_features.shape[0],Train_set_features.shape[1])\n",
    "Y=Train_set_label.values.reshape(Train_set_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez les dimensions de X et de Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta \n",
    "Theta="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta\n",
    "Theta=Grad_Desc(X,Y,eta,K,Grad_Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=\n",
    "MSE=\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=X.dot(Theta)\n",
    "MSE=mean_squared_error(Pred,Y)\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des résultats\n",
    "Comparer vos résultats à ceux obtenus avec ***SGDRegressor***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
