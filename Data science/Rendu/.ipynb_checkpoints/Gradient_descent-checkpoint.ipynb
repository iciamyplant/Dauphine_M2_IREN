{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n",
    "Dans ce notebook, vous allez programmer l'algorithme de descente de gradient dans sa version ordinaire, stochastique et mini-batch.\n",
    "Dans un premier temps, vous utiliserez un jeu de données simple pour tester votre code dans le cas d'une régressison linéaire. Dans un second temps, vous implémenterz vos algorithmes sur un jeu de donnée plus conséquent et comparerez vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un jeu de données simple pour tester votre code\n",
    "Un je de donnée simple constitué de 100 instance est créé artificiellement à l'aide de la relation linéaire $\\ y_{i} = 2+5\\times x_{i}  $<br>\n",
    "Ce jeu de donnée ne comporte qu'une seule variables $\\ x  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un vecteur X de taille 100 \n",
    "X=np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez X ainsi que ses dimensions\n",
    "print('X =', X) #on affiche X\n",
    "print('Dimension =', X.shape) #montre que X est un tableau à une dimension 100x1\n",
    "print('Taille =', len(X)) #donne la taille de X\n",
    "print('Type =', type(X)) #affichons aussi le type de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clcul de Y\n",
    "Y=2+5*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = [  2   7  12  17  22  27  32  37  42  47  52  57  62  67  72  77  82  87\n",
      "  92  97 102 107 112 117 122 127 132 137 142 147 152 157 162 167 172 177\n",
      " 182 187 192 197 202 207 212 217 222 227 232 237 242 247 252 257 262 267\n",
      " 272 277 282 287 292 297 302 307 312 317 322 327 332 337 342 347 352 357\n",
      " 362 367 372 377 382 387 392 397 402 407 412 417 422 427 432 437 442 447\n",
      " 452 457 462 467 472 477 482 487 492 497]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de Y ainsi que ses dimensions\n",
    "print('Y =', Y)\n",
    "print('Dimension =', Y.shape)\n",
    "print('Taille =', len(Y))\n",
    "print('Type =', type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre jeu de données avec les valeurs $\\ x_{i} $ et leur label $\\ y_{i} $ correspondant, il faut transformer le vecteur $\\ X $ poour la faire correpondre à une matrice de forme générale :\n",
    "\n",
    "$$\\begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1j} & ... & x_{1n}\\\\ 1 & x_{21} & x_{22} & ... & x_{2j} & ... & x_{2n} \\\\...&...&...&...&...&...&... \\\\1 & x_{i1} & x_{i2} & ... & x_{ij} & ... & x_{in}\\\\...&...&...&...&...&...&...\\\\1 & x_{m1} & x_{m2} & ... & x_{mj} & ... & x_{mn} \\end{bmatrix}$$ <br>\n",
    "Nous réaliserons cette transformation à l'aide d'une fonction ***Stand_Trans***  qui prend en entrée la matrice $\\ X $ à transformer, le nombre de d'instances $\\ m $ et le nombre de variables $\\ n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stand_Trans(X,m,n):\n",
    "    X=X.reshape(m,n) # Transorme la matrice en entrée en une matrice de dimension m*n\n",
    "    Ones=np.ones((m)).reshape(m,1) #crée une matrice de dimension m*1 remplie de 1\n",
    "    X=np.hstack([Ones,X]) # Concatenation horizontale des matrice X et Ones pour donner lieu à une nouvelle matrice X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez la fonction Stand_Trans au vecteur X\n",
    "X=Stand_Trans(X,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  3.]\n",
      " [ 1.  4.]\n",
      " [ 1.  5.]\n",
      " [ 1.  6.]\n",
      " [ 1.  7.]\n",
      " [ 1.  8.]\n",
      " [ 1.  9.]\n",
      " [ 1. 10.]\n",
      " [ 1. 11.]\n",
      " [ 1. 12.]\n",
      " [ 1. 13.]\n",
      " [ 1. 14.]\n",
      " [ 1. 15.]\n",
      " [ 1. 16.]\n",
      " [ 1. 17.]\n",
      " [ 1. 18.]\n",
      " [ 1. 19.]\n",
      " [ 1. 20.]\n",
      " [ 1. 21.]\n",
      " [ 1. 22.]\n",
      " [ 1. 23.]\n",
      " [ 1. 24.]\n",
      " [ 1. 25.]\n",
      " [ 1. 26.]\n",
      " [ 1. 27.]\n",
      " [ 1. 28.]\n",
      " [ 1. 29.]\n",
      " [ 1. 30.]\n",
      " [ 1. 31.]\n",
      " [ 1. 32.]\n",
      " [ 1. 33.]\n",
      " [ 1. 34.]\n",
      " [ 1. 35.]\n",
      " [ 1. 36.]\n",
      " [ 1. 37.]\n",
      " [ 1. 38.]\n",
      " [ 1. 39.]\n",
      " [ 1. 40.]\n",
      " [ 1. 41.]\n",
      " [ 1. 42.]\n",
      " [ 1. 43.]\n",
      " [ 1. 44.]\n",
      " [ 1. 45.]\n",
      " [ 1. 46.]\n",
      " [ 1. 47.]\n",
      " [ 1. 48.]\n",
      " [ 1. 49.]\n",
      " [ 1. 50.]\n",
      " [ 1. 51.]\n",
      " [ 1. 52.]\n",
      " [ 1. 53.]\n",
      " [ 1. 54.]\n",
      " [ 1. 55.]\n",
      " [ 1. 56.]\n",
      " [ 1. 57.]\n",
      " [ 1. 58.]\n",
      " [ 1. 59.]\n",
      " [ 1. 60.]\n",
      " [ 1. 61.]\n",
      " [ 1. 62.]\n",
      " [ 1. 63.]\n",
      " [ 1. 64.]\n",
      " [ 1. 65.]\n",
      " [ 1. 66.]\n",
      " [ 1. 67.]\n",
      " [ 1. 68.]\n",
      " [ 1. 69.]\n",
      " [ 1. 70.]\n",
      " [ 1. 71.]\n",
      " [ 1. 72.]\n",
      " [ 1. 73.]\n",
      " [ 1. 74.]\n",
      " [ 1. 75.]\n",
      " [ 1. 76.]\n",
      " [ 1. 77.]\n",
      " [ 1. 78.]\n",
      " [ 1. 79.]\n",
      " [ 1. 80.]\n",
      " [ 1. 81.]\n",
      " [ 1. 82.]\n",
      " [ 1. 83.]\n",
      " [ 1. 84.]\n",
      " [ 1. 85.]\n",
      " [ 1. 86.]\n",
      " [ 1. 87.]\n",
      " [ 1. 88.]\n",
      " [ 1. 89.]\n",
      " [ 1. 90.]\n",
      " [ 1. 91.]\n",
      " [ 1. 92.]\n",
      " [ 1. 93.]\n",
      " [ 1. 94.]\n",
      " [ 1. 95.]\n",
      " [ 1. 96.]\n",
      " [ 1. 97.]\n",
      " [ 1. 98.]\n",
      " [ 1. 99.]]\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de la nouvelle matrice X\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez qu'à ce stade votre matrice $\\ X $ est de dimension $\\ 100 \\times 2 $ et que la première colonne colonne ne contient que des $\\ 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation normale\n",
    "Dans le cas de la régression linéaire ($\\ \\hat{Y}= X \\cdot \\theta $ ), l'équation normale donne une solution exacte à la problématique de minimisation de la fonction ***Loss*** lorque celle-ci correspond à une MSE (Mean Squared Error).<br>\n",
    "\\begin{equation}\n",
    "\\theta=(X^T \\times X)^{-1} \\times X^T \\times Y\n",
    "\\end{equation}<br>\n",
    "Pour calculer les $\\ \\theta_{j} $, on utilise les méthodes de calcul matriciel de la bibliothèque ***numpy***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 5.]\n"
     ]
    }
   ],
   "source": [
    "Theta=np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction = 507.00000000000006\n"
     ]
    }
   ],
   "source": [
    "X1 = [1 , 101]\n",
    "prediction = np.dot(X1, Theta)\n",
    "print('prediction =', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des paramètres $\\ \\theta_{j} $ obtenues avec l'équation normale sont optimales. Néanmoins, le calcul peut être long lorsque le nombre de variables est grand. La descente de gradient est une alternative à l'équation normale dans le cas linéaire lorsque le nombre de variables est grand. \n",
    "Par ailleurs, la descente de gradient s'applique à des modèles non-linéaires pour lesuqles il n'existe pas de solution exacte à la problamatique de minimisation de la ***loss***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procédure générique de descente de gradient\n",
    "La descente de gradient est une approche ittérative qui conssiste à mettre à jour les pramètres $\\ \\theta_{j} $ en effectuant un déplacement dans le sens opposé au gradient.<br>\n",
    "De façon générique, la mise à jour des paramètres $\\ \\theta_{j} $ s’effectue comme suit :<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^k=\\theta^{k-1}- \\eta \\times \\nabla_{\\theta}MSE \n",
    "\\end{equation}<br>\n",
    "\n",
    "Avec :<br>\n",
    "- $\\ \\theta $ : le vecteur des paramètres $\\ \\theta_{j} $\n",
    "- $\\ k $ : le numéro d'itération en cours\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ \\nabla_{\\theta}MSE$ : le vecteur gradient <br>\n",
    "\n",
    "*Remarque: l'initialisation de $\\ \\theta $ est arbitraire.*<br>\n",
    "\n",
    "Il existe de nombreuse variantes de la descente de gradient qui se différentient par la façon de calculer $\\ \\nabla_{\\theta}MSE$. <br>\n",
    "Nous pouvons donc écrire une fonction générique de descente de gradient dont un des arguments est lui-même une fonction qui précise la façons de calculer $\\ \\nabla_{\\theta}MSE$.<br>\n",
    "Ecrvivez la fonction ***Grad_Desc*** qui renvoie en sortie $\\ \\theta $ et qui prend comme arguments:<br>\n",
    "- $\\ X $: la matrice des données d'apprentissage\n",
    "- $\\ Y $: le vecteur des labels\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ K $ : le nombre total d'itérations\n",
    "- $\\ Grad $ : la fonction de calcul de $\\ \\nabla_{\\theta}MSE$ <br>\n",
    "\n",
    "*Remarque : dans un premier temps, faites comme si la fonction ***Grad*** était connue. Vous en préciserez le comportement dans un second temps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eta = learning_rate\n",
    "#K = nb_iterations\n",
    "#Grad = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc(X,Y,Eta,K,Grad): #algorithme de gradient\n",
    "    Theta = [7, 8] #random initialisation\n",
    "    for i in range(0, K):\n",
    "        Grad = Grad_Regular(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        Theta = Theta - Eta * Grad #on fait un petit pas\n",
    "        print('i =', i, 'Theta =', Theta)\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient ordinaire\n",
    "Les composantes du vecteur gradient correpondent aux dérivées partielles de la fonction ***Loss*** par rapport à chaque paramètre $\\ \\theta_{j} $. Le vecteur gradient s'écrit comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}MSE=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{0}}\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{1}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{n}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Dans le cas de la descente de gradient ordinaire, la totalité du jeu d'entrainement est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous. <br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i=1}^{m} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= \\dfrac{2}{m} \\times X^T\\cdot(X\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Regular*** qui calcule le gradient ordinaire en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Regular(X,Y,Theta): #calcule le vecteur de gradient\n",
    "    m = len(Y)\n",
    "    Grad = 2/m * X.transpose().dot(X.dot(Theta) - Y)\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Regular***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [ 2.50086739 -0.03770443]\n",
      "Grad = [  307. 20196.]\n",
      "i = 0 Theta = [6.9693 5.9804]\n",
      "Grad = [ 106.9982 6930.2475]\n",
      "i = 1 Theta = [6.95860018 5.28737525]\n",
      "Grad = [  38.36735011 2378.09468457]\n",
      "i = 2 Theta = [6.95476344 5.04956578]\n",
      "Grad = [ 14.81653926 816.02006845]\n",
      "i = 3 Theta = [6.95328179 4.96796377]\n",
      "Grad = [  6.73497728 279.99300576]\n",
      "i = 4 Theta = [6.95260829 4.93996447]\n",
      "Grad = [ 3.96169952 96.0549226 ]\n",
      "i = 5 Theta = [6.95221212 4.93035898]\n",
      "Grad = [ 3.00996345 32.9364341 ]\n",
      "i = 6 Theta = [6.95191113 4.92706534]\n",
      "Grad = [ 2.68329076 11.27727919]\n",
      "i = 7 Theta = [6.9516428  4.92593761]\n",
      "Grad = [2.57110904 3.84492537]\n",
      "i = 8 Theta = [6.95138569 4.92555312]\n",
      "Grad = [2.53253006 1.2945089 ]\n",
      "i = 9 Theta = [6.95113243 4.92542367]\n",
      "Grad = [2.51920791 0.41933286]\n",
      "i = 10 Theta = [6.95088051 4.92538173]\n",
      "Grad = [2.51455267 0.11901681]\n",
      "i = 11 Theta = [6.95062906 4.92536983]\n",
      "Grad = [2.5128715 0.0159644]\n",
      "i = 12 Theta = [6.95037777 4.92536824]\n",
      "Grad = [ 2.51221088 -0.01939685]\n",
      "i = 13 Theta = [6.95012655 4.92537018]\n",
      "Grad = [ 2.51190046 -0.03152983]\n",
      "i = 14 Theta = [6.94987536 4.92537333]\n",
      "Grad = [ 2.51171023 -0.035692  ]\n",
      "i = 15 Theta = [6.94962419 4.9253769 ]\n",
      "Grad = [ 2.51156124 -0.037119  ]\n",
      "i = 16 Theta = [6.94937303 4.92538061]\n",
      "Grad = [ 2.5114264  -0.03760741]\n",
      "i = 17 Theta = [6.94912189 4.92538437]\n",
      "Grad = [ 2.51129643 -0.03777374]\n",
      "i = 18 Theta = [6.94887076 4.92538815]\n",
      "Grad = [ 2.51116813 -0.03782956]\n",
      "i = 19 Theta = [6.94861964 4.92539193]\n",
      "Grad = [ 2.51104041 -0.03784745]\n",
      "i = 20 Theta = [6.94836854 4.92539572]\n",
      "Grad = [ 2.51091289 -0.03785233]\n",
      "i = 21 Theta = [6.94811745 4.9253995 ]\n",
      "Grad = [ 2.51078545 -0.03785274]\n",
      "i = 22 Theta = [6.94786637 4.92540329]\n",
      "Grad = [ 2.51065803 -0.03785162]\n",
      "i = 23 Theta = [6.9476153  4.92540707]\n",
      "Grad = [ 2.51053063 -0.03784998]\n",
      "i = 24 Theta = [6.94736425 4.92541086]\n",
      "Grad = [ 2.51040324 -0.03784815]\n",
      "i = 25 Theta = [6.94711321 4.92541464]\n",
      "Grad = [ 2.51027586 -0.03784626]\n",
      "i = 26 Theta = [6.94686218 4.92541843]\n",
      "Grad = [ 2.51014848 -0.03784435]\n",
      "i = 27 Theta = [6.94661117 4.92542221]\n",
      "Grad = [ 2.51002111 -0.03784244]\n",
      "i = 28 Theta = [6.94636017 4.92542599]\n",
      "Grad = [ 2.50989374 -0.03784052]\n",
      "i = 29 Theta = [6.94610918 4.92542978]\n",
      "Grad = [ 2.50976639 -0.0378386 ]\n",
      "i = 30 Theta = [6.9458582  4.92543356]\n",
      "Grad = [ 2.50963904 -0.03783668]\n",
      "i = 31 Theta = [6.94560724 4.92543735]\n",
      "Grad = [ 2.50951169 -0.03783476]\n",
      "i = 32 Theta = [6.94535628 4.92544113]\n",
      "Grad = [ 2.50938435 -0.03783284]\n",
      "i = 33 Theta = [6.94510535 4.92544491]\n",
      "Grad = [ 2.50925702 -0.03783092]\n",
      "i = 34 Theta = [6.94485442 4.9254487 ]\n",
      "Grad = [ 2.5091297 -0.037829 ]\n",
      "i = 35 Theta = [6.94460351 4.92545248]\n",
      "Grad = [ 2.50900238 -0.03782708]\n",
      "i = 36 Theta = [6.94435261 4.92545626]\n",
      "Grad = [ 2.50887506 -0.03782516]\n",
      "i = 37 Theta = [6.94410172 4.92546004]\n",
      "Grad = [ 2.50874776 -0.03782324]\n",
      "i = 38 Theta = [6.94385085 4.92546383]\n",
      "Grad = [ 2.50862046 -0.03782132]\n",
      "i = 39 Theta = [6.94359998 4.92546761]\n",
      "Grad = [ 2.50849317 -0.0378194 ]\n",
      "i = 40 Theta = [6.94334913 4.92547139]\n",
      "Grad = [ 2.50836588 -0.03781748]\n",
      "i = 41 Theta = [6.9430983  4.92547517]\n",
      "Grad = [ 2.5082386  -0.03781556]\n",
      "i = 42 Theta = [6.94284747 4.92547895]\n",
      "Grad = [ 2.50811133 -0.03781365]\n",
      "i = 43 Theta = [6.94259666 4.92548273]\n",
      "Grad = [ 2.50798406 -0.03781173]\n",
      "i = 44 Theta = [6.94234586 4.92548652]\n",
      "Grad = [ 2.5078568  -0.03780981]\n",
      "i = 45 Theta = [6.94209508 4.9254903 ]\n",
      "Grad = [ 2.50772954 -0.03780789]\n",
      "i = 46 Theta = [6.94184431 4.92549408]\n",
      "Grad = [ 2.5076023  -0.03780597]\n",
      "i = 47 Theta = [6.94159354 4.92549786]\n",
      "Grad = [ 2.50747505 -0.03780405]\n",
      "i = 48 Theta = [6.9413428  4.92550164]\n",
      "Grad = [ 2.50734782 -0.03780213]\n",
      "i = 49 Theta = [6.94109206 4.92550542]\n",
      "Grad = [ 2.50722059 -0.03780022]\n",
      "i = 50 Theta = [6.94084134 4.9255092 ]\n",
      "Grad = [ 2.50709337 -0.0377983 ]\n",
      "i = 51 Theta = [6.94059063 4.92551298]\n",
      "Grad = [ 2.50696615 -0.03779638]\n",
      "i = 52 Theta = [6.94033993 4.92551676]\n",
      "Grad = [ 2.50683894 -0.03779446]\n",
      "i = 53 Theta = [6.94008925 4.92552054]\n",
      "Grad = [ 2.50671174 -0.03779254]\n",
      "i = 54 Theta = [6.93983858 4.92552432]\n",
      "Grad = [ 2.50658455 -0.03779063]\n",
      "i = 55 Theta = [6.93958792 4.9255281 ]\n",
      "Grad = [ 2.50645736 -0.03778871]\n",
      "i = 56 Theta = [6.93933728 4.92553187]\n",
      "Grad = [ 2.50633017 -0.03778679]\n",
      "i = 57 Theta = [6.93908664 4.92553565]\n",
      "Grad = [ 2.506203   -0.03778487]\n",
      "i = 58 Theta = [6.93883602 4.92553943]\n",
      "Grad = [ 2.50607583 -0.03778296]\n",
      "i = 59 Theta = [6.93858541 4.92554321]\n",
      "Grad = [ 2.50594866 -0.03778104]\n",
      "i = 60 Theta = [6.93833482 4.92554699]\n",
      "Grad = [ 2.5058215  -0.03777912]\n",
      "i = 61 Theta = [6.93808424 4.92555077]\n",
      "Grad = [ 2.50569435 -0.03777721]\n",
      "i = 62 Theta = [6.93783367 4.92555454]\n",
      "Grad = [ 2.50556721 -0.03777529]\n",
      "i = 63 Theta = [6.93758311 4.92555832]\n",
      "Grad = [ 2.50544007 -0.03777337]\n",
      "i = 64 Theta = [6.93733257 4.9255621 ]\n",
      "Grad = [ 2.50531294 -0.03777146]\n",
      "i = 65 Theta = [6.93708204 4.92556588]\n",
      "Grad = [ 2.50518581 -0.03776954]\n",
      "i = 66 Theta = [6.93683152 4.92556965]\n",
      "Grad = [ 2.5050587  -0.03776762]\n",
      "i = 67 Theta = [6.93658101 4.92557343]\n",
      "Grad = [ 2.50493158 -0.03776571]\n",
      "i = 68 Theta = [6.93633052 4.92557721]\n",
      "Grad = [ 2.50480448 -0.03776379]\n",
      "i = 69 Theta = [6.93608004 4.92558098]\n",
      "Grad = [ 2.50467738 -0.03776187]\n",
      "i = 70 Theta = [6.93582957 4.92558476]\n",
      "Grad = [ 2.50455029 -0.03775996]\n",
      "i = 71 Theta = [6.93557912 4.92558854]\n",
      "Grad = [ 2.5044232  -0.03775804]\n",
      "i = 72 Theta = [6.93532867 4.92559231]\n",
      "Grad = [ 2.50429612 -0.03775613]\n",
      "i = 73 Theta = [6.93507824 4.92559609]\n",
      "Grad = [ 2.50416904 -0.03775421]\n",
      "i = 74 Theta = [6.93482783 4.92559986]\n",
      "Grad = [ 2.50404198 -0.03775229]\n",
      "i = 75 Theta = [6.93457742 4.92560364]\n",
      "Grad = [ 2.50391492 -0.03775038]\n",
      "i = 76 Theta = [6.93432703 4.92560741]\n",
      "Grad = [ 2.50378786 -0.03774846]\n",
      "i = 77 Theta = [6.93407665 4.92561119]\n",
      "Grad = [ 2.50366082 -0.03774655]\n",
      "i = 78 Theta = [6.93382629 4.92561496]\n",
      "Grad = [ 2.50353377 -0.03774463]\n",
      "i = 79 Theta = [6.93357593 4.92561874]\n",
      "Grad = [ 2.50340674 -0.03774272]\n",
      "i = 80 Theta = [6.93332559 4.92562251]\n",
      "Grad = [ 2.50327971 -0.0377408 ]\n",
      "i = 81 Theta = [6.93307526 4.92562628]\n",
      "Grad = [ 2.50315269 -0.03773889]\n",
      "i = 82 Theta = [6.93282495 4.92563006]\n",
      "Grad = [ 2.50302567 -0.03773697]\n",
      "i = 83 Theta = [6.93257465 4.92563383]\n",
      "Grad = [ 2.50289866 -0.03773506]\n",
      "i = 84 Theta = [6.93232436 4.92563761]\n",
      "Grad = [ 2.50277166 -0.03773314]\n",
      "i = 85 Theta = [6.93207408 4.92564138]\n",
      "Grad = [ 2.50264466 -0.03773123]\n",
      "i = 86 Theta = [6.93182381 4.92564515]\n",
      "Grad = [ 2.50251767 -0.03772931]\n",
      "i = 87 Theta = [6.93157356 4.92564892]\n",
      "Grad = [ 2.50239069 -0.0377274 ]\n",
      "i = 88 Theta = [6.93132332 4.9256527 ]\n",
      "Grad = [ 2.50226371 -0.03772548]\n",
      "i = 89 Theta = [6.9310731  4.92565647]\n",
      "Grad = [ 2.50213674 -0.03772357]\n",
      "i = 90 Theta = [6.93082288 4.92566024]\n",
      "Grad = [ 2.50200978 -0.03772166]\n",
      "i = 91 Theta = [6.93057268 4.92566401]\n",
      "Grad = [ 2.50188282 -0.03771974]\n",
      "i = 92 Theta = [6.93032249 4.92566779]\n",
      "Grad = [ 2.50175587 -0.03771783]\n",
      "i = 93 Theta = [6.93007232 4.92567156]\n",
      "Grad = [ 2.50162893 -0.03771591]\n",
      "i = 94 Theta = [6.92982216 4.92567533]\n",
      "Grad = [ 2.50150199 -0.037714  ]\n",
      "i = 95 Theta = [6.92957201 4.9256791 ]\n",
      "Grad = [ 2.50137506 -0.03771209]\n",
      "i = 96 Theta = [6.92932187 4.92568287]\n",
      "Grad = [ 2.50124813 -0.03771017]\n",
      "i = 97 Theta = [6.92907174 4.92568664]\n",
      "Grad = [ 2.50112121 -0.03770826]\n",
      "i = 98 Theta = [6.92882163 4.92569041]\n",
      "Grad = [ 2.5009943  -0.03770635]\n",
      "i = 99 Theta = [6.92857153 4.92569419]\n"
     ]
    }
   ],
   "source": [
    "Theta = Grad_Desc(X,Y,0.0001,100,Grad_Regular(X, Y, Theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction2 = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "X2 = [1 , 101]\n",
    "prediction2 = np.dot(X2, Theta)\n",
    "print('prediction2 =', prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient stochastique\n",
    "Dans le cas de la descente de gradient stochastique, seule une instance $\\ X_{i} $ du jeu d'entrainement, tirée au hasard à chaque itération, est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous.<br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=2 \\times x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Pour une instance $\\ i$ prise au hasard, le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= 2 \\times X_{i}\\cdot(X_{i}\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Stochastic*** qui calcule le gradient stochastique en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # à utiliser pour le tirage aléatoire d'une instance Xi à chaque itération\n",
    "def Grad_Stochastic(X,Y,Theta):\n",
    "    i = random.randrange(0, 99, 1)\n",
    "    Grad = 2 * X[i].dot(X[i].dot(Theta) - Y)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient stochastique sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Stochastic***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grad_Desc(X,Y,Eta,K,Grad_Stochastic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la descente de gradient mini-batch, un sous-ensemble du jeu d'entrainement, constitué de $\\ q $ exemples tirés aléatoirement, est utilisé à chaque itération pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$. Le nombre $\\ q $  est un hyperparamètre fixé par l'utilisateur avec $\\ q \\le m$.<br>\n",
    "\n",
    "La formule pour le calcul $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ devient:\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i\\in \\mathscr{E}} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Avec $\\mathscr{E} $ l'ensemble des $\\ q $ exemples tirés aléatoirement (cet ensemble est réactualisé à cahque itération).\n",
    "\n",
    "Ecrivez la fonction ***Grad_Batch*** qui calcule le gradient mini_batch en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Batch(X,Y,Theta,q):\n",
    "    # Votre code ici\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Batch***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$\n",
    "- $\\ q=10$\n",
    "\n",
    "*Remarque : pensez à adapter la fonction ***Grad_Desc*** en ***Grad_Desc_Batch***  pour tenir compte de l'hyperparamètre $\\ q $ spécifique à l'approche mini-batch. Dans la nouvelle fonction ***Grad_Desc_Batch***, le paramètre ***Grad*** prend par défaut la valeur ***Grad_Batch***.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc_Batch(X,Y,Eta,K,q,Grad=Grad_Batch):\n",
    "    # Votre code ici\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grad_Desc_Batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests sur jeu de donnée plus conséquent\n",
    "Vous allez maintenant tester votre code sur un jeu de données plus conséquent et comparer vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***.<br>\n",
    "L'objectif est d'entrainer un régresseur linéaire qui prédit les dépenses de santé par ménage à partir de la base de données HISP (Health Insurance Subsidy Program). La base HISP contient des informations concernant les dépenses de santé de ménages américains avant et après la mise en plce du programme d'assurance HISP (les données sont fictives). <br>\n",
    "\n",
    "\n",
    "Pour des raisons de cohérence, vous n'utiliserez que les données décrivant la situation des ménages avant la mise en œuvre du programme HISP (rond=Before).<br>\n",
    "De plus, seules les variables continues ci-dessous seront utilisées pour entraîner votre modèle :\n",
    "- poverty_index \n",
    "- hhsize\n",
    "- age_sp\n",
    "- age_hh\n",
    "- educ_hh\n",
    "- educ_sp\n",
    "- hospital_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et nettoyage des données\n",
    "Comme il s'agit simplement de comparer vos résultats avec ceux obtenus avec les regérsseurs ScikitLearn, il n'est pas nécessaire de séparer les données en jeu de test et jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"datafinalexam.csv\") # importation des données (le fichier datafinalexam.csv et le notebook doivent être dans le même dossier)\n",
    "data=data.loc[data[\"round\"]==\"Before\"] # filtre pour garder la situation avant la mise en place du dispositif\n",
    "features=[\"poverty_index\",\"hhsize\",\"age_sp\",\"age_hh\",\"educ_hh\",\"educ_sp\",\"hospital_distance\"] # liste des variables à utiliser\n",
    "label=['health_expenditures'] # label\n",
    "data=data[features+label] # selction des colonnes utiles\n",
    "Train_set_features=data[features] # variables du jeu d'entrainement \n",
    "Train_set_label=data[label] # labeldu jeu d'entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres\n",
    "eta=0.00001\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mustapha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=1e-05, learning_rate='constant')"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor # import de la classe SGDRegressor\n",
    "reg = SGDRegressor(fit_intercept=True,learning_rate=\"constant\",eta0=eta,max_iter=K) # Instancition d'un régresseur\n",
    "reg.fit(Train_set_features,Train_set_label) # entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37200136  0.26348105 -1.34086898  0.02423312  0.12008257  0.25357157\n",
      "  0.26443865  0.00662281]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient du modèle linéaire\n",
    "Theta=np.concatenate((reg.intercept_,reg.coef_),axis=None)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.941030553940154\n"
     ]
    }
   ],
   "source": [
    "# Performance du modèle Scikit-Learn (RMSE sur jeu d'entrainement)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "Pred=reg.predict(Train_set_features) # prédiction faites par le modèle\n",
    "MSE=mean_squared_error(Pred,Train_set_label) # MSE entre prédictions et valeurs réelles\n",
    "RMSE=MSE**(1/2) # RMSE\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - Votre approche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données dans un format compatible avec vos fonctions\n",
    "X=Stand_Trans(Train_set_features.values,Train_set_features.shape[0],Train_set_features.shape[1])\n",
    "Y=Train_set_label.values.reshape(Train_set_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez les dimensions de X et de Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta \n",
    "Theta="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta\n",
    "Theta=Grad_Desc(X,Y,eta,K,Grad_Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=\n",
    "MSE=\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=X.dot(Theta)\n",
    "MSE=mean_squared_error(Pred,Y)\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des résultats\n",
    "Comparer vos résultats à ceux obtenus avec ***SGDRegressor***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
