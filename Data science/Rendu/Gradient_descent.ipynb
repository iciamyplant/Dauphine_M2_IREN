{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n",
    "Dans ce notebook, vous allez programmer l'algorithme de descente de gradient dans sa version ordinaire, stochastique et mini-batch.\n",
    "Dans un premier temps, vous utiliserez un jeu de données simple pour tester votre code dans le cas d'une régressison linéaire. Dans un second temps, vous implémenterz vos algorithmes sur un jeu de donnée plus conséquent et comparerez vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un jeu de données simple pour tester votre code\n",
    "Un je de donnée simple constitué de 100 instance est créé artificiellement à l'aide de la relation linéaire $\\ y_{i} = 2+5\\times x_{i}  $<br>\n",
    "Ce jeu de donnée ne comporte qu'une seule variables $\\ x  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un vecteur X de taille 100 \n",
    "X=np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez X ainsi que ses dimensions\n",
    "print('X =', X) #on affiche X\n",
    "print('Dimension =', X.shape) #montre que X est un tableau à une dimension 100x1\n",
    "print('Taille =', len(X)) #donne la taille de X\n",
    "print('Type =', type(X)) #affichons aussi le type de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clcul de Y\n",
    "Y=2+5*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = [  2   7  12  17  22  27  32  37  42  47  52  57  62  67  72  77  82  87\n",
      "  92  97 102 107 112 117 122 127 132 137 142 147 152 157 162 167 172 177\n",
      " 182 187 192 197 202 207 212 217 222 227 232 237 242 247 252 257 262 267\n",
      " 272 277 282 287 292 297 302 307 312 317 322 327 332 337 342 347 352 357\n",
      " 362 367 372 377 382 387 392 397 402 407 412 417 422 427 432 437 442 447\n",
      " 452 457 462 467 472 477 482 487 492 497]\n",
      "Dimension = (100,)\n",
      "Taille = 100\n",
      "Type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de Y ainsi que ses dimensions\n",
    "print('Y =', Y)\n",
    "print('Dimension =', Y.shape)\n",
    "print('Taille =', len(Y))\n",
    "print('Type =', type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre jeu de données avec les valeurs $\\ x_{i} $ et leur label $\\ y_{i} $ correspondant, il faut transformer le vecteur $\\ X $ poour la faire correpondre à une matrice de forme générale :\n",
    "\n",
    "$$\\begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1j} & ... & x_{1n}\\\\ 1 & x_{21} & x_{22} & ... & x_{2j} & ... & x_{2n} \\\\...&...&...&...&...&...&... \\\\1 & x_{i1} & x_{i2} & ... & x_{ij} & ... & x_{in}\\\\...&...&...&...&...&...&...\\\\1 & x_{m1} & x_{m2} & ... & x_{mj} & ... & x_{mn} \\end{bmatrix}$$ <br>\n",
    "Nous réaliserons cette transformation à l'aide d'une fonction ***Stand_Trans***  qui prend en entrée la matrice $\\ X $ à transformer, le nombre de d'instances $\\ m $ et le nombre de variables $\\ n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stand_Trans(X,m,n):\n",
    "    X=X.reshape(m,n) # Transorme la matrice en entrée en une matrice de dimension m*n\n",
    "    Ones=np.ones((m)).reshape(m,1) #crée une matrice de dimension m*1 remplie de 1\n",
    "    X=np.hstack([Ones,X]) # Concatenation horizontale des matrice X et Ones pour donner lieu à une nouvelle matrice X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez la fonction Stand_Trans au vecteur X\n",
    "X=Stand_Trans(X,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  3.]\n",
      " [ 1.  4.]\n",
      " [ 1.  5.]\n",
      " [ 1.  6.]\n",
      " [ 1.  7.]\n",
      " [ 1.  8.]\n",
      " [ 1.  9.]\n",
      " [ 1. 10.]\n",
      " [ 1. 11.]\n",
      " [ 1. 12.]\n",
      " [ 1. 13.]\n",
      " [ 1. 14.]\n",
      " [ 1. 15.]\n",
      " [ 1. 16.]\n",
      " [ 1. 17.]\n",
      " [ 1. 18.]\n",
      " [ 1. 19.]\n",
      " [ 1. 20.]\n",
      " [ 1. 21.]\n",
      " [ 1. 22.]\n",
      " [ 1. 23.]\n",
      " [ 1. 24.]\n",
      " [ 1. 25.]\n",
      " [ 1. 26.]\n",
      " [ 1. 27.]\n",
      " [ 1. 28.]\n",
      " [ 1. 29.]\n",
      " [ 1. 30.]\n",
      " [ 1. 31.]\n",
      " [ 1. 32.]\n",
      " [ 1. 33.]\n",
      " [ 1. 34.]\n",
      " [ 1. 35.]\n",
      " [ 1. 36.]\n",
      " [ 1. 37.]\n",
      " [ 1. 38.]\n",
      " [ 1. 39.]\n",
      " [ 1. 40.]\n",
      " [ 1. 41.]\n",
      " [ 1. 42.]\n",
      " [ 1. 43.]\n",
      " [ 1. 44.]\n",
      " [ 1. 45.]\n",
      " [ 1. 46.]\n",
      " [ 1. 47.]\n",
      " [ 1. 48.]\n",
      " [ 1. 49.]\n",
      " [ 1. 50.]\n",
      " [ 1. 51.]\n",
      " [ 1. 52.]\n",
      " [ 1. 53.]\n",
      " [ 1. 54.]\n",
      " [ 1. 55.]\n",
      " [ 1. 56.]\n",
      " [ 1. 57.]\n",
      " [ 1. 58.]\n",
      " [ 1. 59.]\n",
      " [ 1. 60.]\n",
      " [ 1. 61.]\n",
      " [ 1. 62.]\n",
      " [ 1. 63.]\n",
      " [ 1. 64.]\n",
      " [ 1. 65.]\n",
      " [ 1. 66.]\n",
      " [ 1. 67.]\n",
      " [ 1. 68.]\n",
      " [ 1. 69.]\n",
      " [ 1. 70.]\n",
      " [ 1. 71.]\n",
      " [ 1. 72.]\n",
      " [ 1. 73.]\n",
      " [ 1. 74.]\n",
      " [ 1. 75.]\n",
      " [ 1. 76.]\n",
      " [ 1. 77.]\n",
      " [ 1. 78.]\n",
      " [ 1. 79.]\n",
      " [ 1. 80.]\n",
      " [ 1. 81.]\n",
      " [ 1. 82.]\n",
      " [ 1. 83.]\n",
      " [ 1. 84.]\n",
      " [ 1. 85.]\n",
      " [ 1. 86.]\n",
      " [ 1. 87.]\n",
      " [ 1. 88.]\n",
      " [ 1. 89.]\n",
      " [ 1. 90.]\n",
      " [ 1. 91.]\n",
      " [ 1. 92.]\n",
      " [ 1. 93.]\n",
      " [ 1. 94.]\n",
      " [ 1. 95.]\n",
      " [ 1. 96.]\n",
      " [ 1. 97.]\n",
      " [ 1. 98.]\n",
      " [ 1. 99.]]\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de la nouvelle matrice X\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez qu'à ce stade votre matrice $\\ X $ est de dimension $\\ 100 \\times 2 $ et que la première colonne colonne ne contient que des $\\ 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation normale\n",
    "Dans le cas de la régression linéaire ($\\ \\hat{Y}= X \\cdot \\theta $ ), l'équation normale donne une solution exacte à la problématique de minimisation de la fonction ***Loss*** lorque celle-ci correspond à une MSE (Mean Squared Error).<br>\n",
    "\\begin{equation}\n",
    "\\theta=(X^T \\times X)^{-1} \\times X^T \\times Y\n",
    "\\end{equation}<br>\n",
    "Pour calculer les $\\ \\theta_{j} $, on utilise les méthodes de calcul matriciel de la bibliothèque ***numpy***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 5.]\n"
     ]
    }
   ],
   "source": [
    "Theta=np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction = 507.00000000000006\n"
     ]
    }
   ],
   "source": [
    "X1 = [1 , 101]\n",
    "prediction = np.dot(X1, Theta)\n",
    "print('prediction =', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des paramètres $\\ \\theta_{j} $ obtenues avec l'équation normale sont optimales. Néanmoins, le calcul peut être long lorsque le nombre de variables est grand. La descente de gradient est une alternative à l'équation normale dans le cas linéaire lorsque le nombre de variables est grand. \n",
    "Par ailleurs, la descente de gradient s'applique à des modèles non-linéaires pour lesuqles il n'existe pas de solution exacte à la problamatique de minimisation de la ***loss***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procédure générique de descente de gradient\n",
    "La descente de gradient est une approche ittérative qui conssiste à mettre à jour les pramètres $\\ \\theta_{j} $ en effectuant un déplacement dans le sens opposé au gradient.<br>\n",
    "De façon générique, la mise à jour des paramètres $\\ \\theta_{j} $ s’effectue comme suit :<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^k=\\theta^{k-1}- \\eta \\times \\nabla_{\\theta}MSE \n",
    "\\end{equation}<br>\n",
    "\n",
    "Avec :<br>\n",
    "- $\\ \\theta $ : le vecteur des paramètres $\\ \\theta_{j} $\n",
    "- $\\ k $ : le numéro d'itération en cours\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ \\nabla_{\\theta}MSE$ : le vecteur gradient <br>\n",
    "\n",
    "*Remarque: l'initialisation de $\\ \\theta $ est arbitraire.*<br>\n",
    "\n",
    "Il existe de nombreuse variantes de la descente de gradient qui se différentient par la façon de calculer $\\ \\nabla_{\\theta}MSE$. <br>\n",
    "Nous pouvons donc écrire une fonction générique de descente de gradient dont un des arguments est lui-même une fonction qui précise la façons de calculer $\\ \\nabla_{\\theta}MSE$.<br>\n",
    "Ecrvivez la fonction ***Grad_Desc*** qui renvoie en sortie $\\ \\theta $ et qui prend comme arguments:<br>\n",
    "- $\\ X $: la matrice des données d'apprentissage\n",
    "- $\\ Y $: le vecteur des labels\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ K $ : le nombre total d'itérations\n",
    "- $\\ Grad $ : la fonction de calcul de $\\ \\nabla_{\\theta}MSE$ <br>\n",
    "\n",
    "*Remarque : dans un premier temps, faites comme si la fonction ***Grad*** était connue. Vous en préciserez le comportement dans un second temps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eta = learning_rate\n",
    "#K = nb_iterations\n",
    "#Grad = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc(X,Y,Eta,K,Grad_function): #algorithme de gradient\n",
    "    Theta = [7, 8] #random initialisation\n",
    "    for i in range(0, K):\n",
    "        if Grad_function == 'stochastic':\n",
    "            Grad = Grad_Stochastic(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        elif Grad_function == 'regular':\n",
    "            Grad = Grad_Regular(X,Y,Theta) #on recalcule la dérivée à chaque fois\n",
    "        else :\n",
    "            print('Syntax Error :\"', Grad_function, '\"')\n",
    "            return\n",
    "        Theta = Theta - Eta * Grad #on fait un petit pas\n",
    "        print('i =', i, 'Theta =', Theta)\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient ordinaire\n",
    "Les composantes du vecteur gradient correpondent aux dérivées partielles de la fonction ***Loss*** par rapport à chaque paramètre $\\ \\theta_{j} $. Le vecteur gradient s'écrit comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}MSE=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{0}}\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{1}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{n}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Dans le cas de la descente de gradient ordinaire, la totalité du jeu d'entrainement est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous. <br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i=1}^{m} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= \\dfrac{2}{m} \\times X^T\\cdot(X\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Regular*** qui calcule le gradient ordinaire en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Regular(X,Y,Theta): #calcule le vecteur de gradient\n",
    "    m = len(Y)\n",
    "    Grad = 2/m * X.transpose().dot(X.dot(Theta) - Y)\n",
    "    #print('TYPE==================', type(Theta))\n",
    "    #print('TYPE==================', type(X))\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Regular***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [  307. 20196.]\n",
      "i = 0 Theta = [6.9693 5.9804]\n",
      "Grad = [ 106.9982 6930.2475]\n",
      "i = 1 Theta = [6.95860018 5.28737525]\n",
      "Grad = [  38.36735011 2378.09468457]\n",
      "i = 2 Theta = [6.95476344 5.04956578]\n",
      "Grad = [ 14.81653926 816.02006845]\n",
      "i = 3 Theta = [6.95328179 4.96796377]\n",
      "Grad = [  6.73497728 279.99300576]\n",
      "i = 4 Theta = [6.95260829 4.93996447]\n",
      "Grad = [ 3.96169952 96.0549226 ]\n",
      "i = 5 Theta = [6.95221212 4.93035898]\n",
      "Grad = [ 3.00996345 32.9364341 ]\n",
      "i = 6 Theta = [6.95191113 4.92706534]\n",
      "Grad = [ 2.68329076 11.27727919]\n",
      "i = 7 Theta = [6.9516428  4.92593761]\n",
      "Grad = [2.57110904 3.84492537]\n",
      "i = 8 Theta = [6.95138569 4.92555312]\n",
      "Grad = [2.53253006 1.2945089 ]\n",
      "i = 9 Theta = [6.95113243 4.92542367]\n",
      "Grad = [2.51920791 0.41933286]\n",
      "i = 10 Theta = [6.95088051 4.92538173]\n",
      "Grad = [2.51455267 0.11901681]\n",
      "i = 11 Theta = [6.95062906 4.92536983]\n",
      "Grad = [2.5128715 0.0159644]\n",
      "i = 12 Theta = [6.95037777 4.92536824]\n",
      "Grad = [ 2.51221088 -0.01939685]\n",
      "i = 13 Theta = [6.95012655 4.92537018]\n",
      "Grad = [ 2.51190046 -0.03152983]\n",
      "i = 14 Theta = [6.94987536 4.92537333]\n",
      "Grad = [ 2.51171023 -0.035692  ]\n",
      "i = 15 Theta = [6.94962419 4.9253769 ]\n",
      "Grad = [ 2.51156124 -0.037119  ]\n",
      "i = 16 Theta = [6.94937303 4.92538061]\n",
      "Grad = [ 2.5114264  -0.03760741]\n",
      "i = 17 Theta = [6.94912189 4.92538437]\n",
      "Grad = [ 2.51129643 -0.03777374]\n",
      "i = 18 Theta = [6.94887076 4.92538815]\n",
      "Grad = [ 2.51116813 -0.03782956]\n",
      "i = 19 Theta = [6.94861964 4.92539193]\n",
      "Grad = [ 2.51104041 -0.03784745]\n",
      "i = 20 Theta = [6.94836854 4.92539572]\n",
      "Grad = [ 2.51091289 -0.03785233]\n",
      "i = 21 Theta = [6.94811745 4.9253995 ]\n",
      "Grad = [ 2.51078545 -0.03785274]\n",
      "i = 22 Theta = [6.94786637 4.92540329]\n",
      "Grad = [ 2.51065803 -0.03785162]\n",
      "i = 23 Theta = [6.9476153  4.92540707]\n",
      "Grad = [ 2.51053063 -0.03784998]\n",
      "i = 24 Theta = [6.94736425 4.92541086]\n",
      "Grad = [ 2.51040324 -0.03784815]\n",
      "i = 25 Theta = [6.94711321 4.92541464]\n",
      "Grad = [ 2.51027586 -0.03784626]\n",
      "i = 26 Theta = [6.94686218 4.92541843]\n",
      "Grad = [ 2.51014848 -0.03784435]\n",
      "i = 27 Theta = [6.94661117 4.92542221]\n",
      "Grad = [ 2.51002111 -0.03784244]\n",
      "i = 28 Theta = [6.94636017 4.92542599]\n",
      "Grad = [ 2.50989374 -0.03784052]\n",
      "i = 29 Theta = [6.94610918 4.92542978]\n",
      "Grad = [ 2.50976639 -0.0378386 ]\n",
      "i = 30 Theta = [6.9458582  4.92543356]\n",
      "Grad = [ 2.50963904 -0.03783668]\n",
      "i = 31 Theta = [6.94560724 4.92543735]\n",
      "Grad = [ 2.50951169 -0.03783476]\n",
      "i = 32 Theta = [6.94535628 4.92544113]\n",
      "Grad = [ 2.50938435 -0.03783284]\n",
      "i = 33 Theta = [6.94510535 4.92544491]\n",
      "Grad = [ 2.50925702 -0.03783092]\n",
      "i = 34 Theta = [6.94485442 4.9254487 ]\n",
      "Grad = [ 2.5091297 -0.037829 ]\n",
      "i = 35 Theta = [6.94460351 4.92545248]\n",
      "Grad = [ 2.50900238 -0.03782708]\n",
      "i = 36 Theta = [6.94435261 4.92545626]\n",
      "Grad = [ 2.50887506 -0.03782516]\n",
      "i = 37 Theta = [6.94410172 4.92546004]\n",
      "Grad = [ 2.50874776 -0.03782324]\n",
      "i = 38 Theta = [6.94385085 4.92546383]\n",
      "Grad = [ 2.50862046 -0.03782132]\n",
      "i = 39 Theta = [6.94359998 4.92546761]\n",
      "Grad = [ 2.50849317 -0.0378194 ]\n",
      "i = 40 Theta = [6.94334913 4.92547139]\n",
      "Grad = [ 2.50836588 -0.03781748]\n",
      "i = 41 Theta = [6.9430983  4.92547517]\n",
      "Grad = [ 2.5082386  -0.03781556]\n",
      "i = 42 Theta = [6.94284747 4.92547895]\n",
      "Grad = [ 2.50811133 -0.03781365]\n",
      "i = 43 Theta = [6.94259666 4.92548273]\n",
      "Grad = [ 2.50798406 -0.03781173]\n",
      "i = 44 Theta = [6.94234586 4.92548652]\n",
      "Grad = [ 2.5078568  -0.03780981]\n",
      "i = 45 Theta = [6.94209508 4.9254903 ]\n",
      "Grad = [ 2.50772954 -0.03780789]\n",
      "i = 46 Theta = [6.94184431 4.92549408]\n",
      "Grad = [ 2.5076023  -0.03780597]\n",
      "i = 47 Theta = [6.94159354 4.92549786]\n",
      "Grad = [ 2.50747505 -0.03780405]\n",
      "i = 48 Theta = [6.9413428  4.92550164]\n",
      "Grad = [ 2.50734782 -0.03780213]\n",
      "i = 49 Theta = [6.94109206 4.92550542]\n",
      "Grad = [ 2.50722059 -0.03780022]\n",
      "i = 50 Theta = [6.94084134 4.9255092 ]\n",
      "Grad = [ 2.50709337 -0.0377983 ]\n",
      "i = 51 Theta = [6.94059063 4.92551298]\n",
      "Grad = [ 2.50696615 -0.03779638]\n",
      "i = 52 Theta = [6.94033993 4.92551676]\n",
      "Grad = [ 2.50683894 -0.03779446]\n",
      "i = 53 Theta = [6.94008925 4.92552054]\n",
      "Grad = [ 2.50671174 -0.03779254]\n",
      "i = 54 Theta = [6.93983858 4.92552432]\n",
      "Grad = [ 2.50658455 -0.03779063]\n",
      "i = 55 Theta = [6.93958792 4.9255281 ]\n",
      "Grad = [ 2.50645736 -0.03778871]\n",
      "i = 56 Theta = [6.93933728 4.92553187]\n",
      "Grad = [ 2.50633017 -0.03778679]\n",
      "i = 57 Theta = [6.93908664 4.92553565]\n",
      "Grad = [ 2.506203   -0.03778487]\n",
      "i = 58 Theta = [6.93883602 4.92553943]\n",
      "Grad = [ 2.50607583 -0.03778296]\n",
      "i = 59 Theta = [6.93858541 4.92554321]\n",
      "Grad = [ 2.50594866 -0.03778104]\n",
      "i = 60 Theta = [6.93833482 4.92554699]\n",
      "Grad = [ 2.5058215  -0.03777912]\n",
      "i = 61 Theta = [6.93808424 4.92555077]\n",
      "Grad = [ 2.50569435 -0.03777721]\n",
      "i = 62 Theta = [6.93783367 4.92555454]\n",
      "Grad = [ 2.50556721 -0.03777529]\n",
      "i = 63 Theta = [6.93758311 4.92555832]\n",
      "Grad = [ 2.50544007 -0.03777337]\n",
      "i = 64 Theta = [6.93733257 4.9255621 ]\n",
      "Grad = [ 2.50531294 -0.03777146]\n",
      "i = 65 Theta = [6.93708204 4.92556588]\n",
      "Grad = [ 2.50518581 -0.03776954]\n",
      "i = 66 Theta = [6.93683152 4.92556965]\n",
      "Grad = [ 2.5050587  -0.03776762]\n",
      "i = 67 Theta = [6.93658101 4.92557343]\n",
      "Grad = [ 2.50493158 -0.03776571]\n",
      "i = 68 Theta = [6.93633052 4.92557721]\n",
      "Grad = [ 2.50480448 -0.03776379]\n",
      "i = 69 Theta = [6.93608004 4.92558098]\n",
      "Grad = [ 2.50467738 -0.03776187]\n",
      "i = 70 Theta = [6.93582957 4.92558476]\n",
      "Grad = [ 2.50455029 -0.03775996]\n",
      "i = 71 Theta = [6.93557912 4.92558854]\n",
      "Grad = [ 2.5044232  -0.03775804]\n",
      "i = 72 Theta = [6.93532867 4.92559231]\n",
      "Grad = [ 2.50429612 -0.03775613]\n",
      "i = 73 Theta = [6.93507824 4.92559609]\n",
      "Grad = [ 2.50416904 -0.03775421]\n",
      "i = 74 Theta = [6.93482783 4.92559986]\n",
      "Grad = [ 2.50404198 -0.03775229]\n",
      "i = 75 Theta = [6.93457742 4.92560364]\n",
      "Grad = [ 2.50391492 -0.03775038]\n",
      "i = 76 Theta = [6.93432703 4.92560741]\n",
      "Grad = [ 2.50378786 -0.03774846]\n",
      "i = 77 Theta = [6.93407665 4.92561119]\n",
      "Grad = [ 2.50366082 -0.03774655]\n",
      "i = 78 Theta = [6.93382629 4.92561496]\n",
      "Grad = [ 2.50353377 -0.03774463]\n",
      "i = 79 Theta = [6.93357593 4.92561874]\n",
      "Grad = [ 2.50340674 -0.03774272]\n",
      "i = 80 Theta = [6.93332559 4.92562251]\n",
      "Grad = [ 2.50327971 -0.0377408 ]\n",
      "i = 81 Theta = [6.93307526 4.92562628]\n",
      "Grad = [ 2.50315269 -0.03773889]\n",
      "i = 82 Theta = [6.93282495 4.92563006]\n",
      "Grad = [ 2.50302567 -0.03773697]\n",
      "i = 83 Theta = [6.93257465 4.92563383]\n",
      "Grad = [ 2.50289866 -0.03773506]\n",
      "i = 84 Theta = [6.93232436 4.92563761]\n",
      "Grad = [ 2.50277166 -0.03773314]\n",
      "i = 85 Theta = [6.93207408 4.92564138]\n",
      "Grad = [ 2.50264466 -0.03773123]\n",
      "i = 86 Theta = [6.93182381 4.92564515]\n",
      "Grad = [ 2.50251767 -0.03772931]\n",
      "i = 87 Theta = [6.93157356 4.92564892]\n",
      "Grad = [ 2.50239069 -0.0377274 ]\n",
      "i = 88 Theta = [6.93132332 4.9256527 ]\n",
      "Grad = [ 2.50226371 -0.03772548]\n",
      "i = 89 Theta = [6.9310731  4.92565647]\n",
      "Grad = [ 2.50213674 -0.03772357]\n",
      "i = 90 Theta = [6.93082288 4.92566024]\n",
      "Grad = [ 2.50200978 -0.03772166]\n",
      "i = 91 Theta = [6.93057268 4.92566401]\n",
      "Grad = [ 2.50188282 -0.03771974]\n",
      "i = 92 Theta = [6.93032249 4.92566779]\n",
      "Grad = [ 2.50175587 -0.03771783]\n",
      "i = 93 Theta = [6.93007232 4.92567156]\n",
      "Grad = [ 2.50162893 -0.03771591]\n",
      "i = 94 Theta = [6.92982216 4.92567533]\n",
      "Grad = [ 2.50150199 -0.037714  ]\n",
      "i = 95 Theta = [6.92957201 4.9256791 ]\n",
      "Grad = [ 2.50137506 -0.03771209]\n",
      "i = 96 Theta = [6.92932187 4.92568287]\n",
      "Grad = [ 2.50124813 -0.03771017]\n",
      "i = 97 Theta = [6.92907174 4.92568664]\n",
      "Grad = [ 2.50112121 -0.03770826]\n",
      "i = 98 Theta = [6.92882163 4.92569041]\n",
      "Grad = [ 2.5009943  -0.03770635]\n",
      "i = 99 Theta = [6.92857153 4.92569419]\n"
     ]
    }
   ],
   "source": [
    "Theta = Grad_Desc(X,Y,0.0001,100,'regular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_regular = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_regular = np.dot(X1, Theta)\n",
    "print('prediction_regular =', prediction_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient stochastique\n",
    "Dans le cas de la descente de gradient stochastique, seule une instance $\\ X_{i} $ du jeu d'entrainement, tirée au hasard à chaque itération, est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous.<br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=2 \\times x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Pour une instance $\\ i$ prise au hasard, le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= 2 \\times X_{i}\\cdot(X_{i}\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Stochastic*** qui calcule le gradient stochastique en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # à utiliser pour le tirage aléatoire d'une instance Xi à chaque itération\n",
    "def Grad_Stochastic(X,Y,Theta):\n",
    "    i = random.randrange(0, 99, 1)\n",
    "    #Xi = Xi.reshape(2, 1)\n",
    "    #Theta = np.reshape(Theta, (2, 1))\n",
    "    Grad = 2 * X[i].dot(X[i].dot(Theta) - Y[i]) #impossible avec X.shape(2,) et Y(100,) donc j'ai mis Y[i]\n",
    "    print('Grad =', Grad)\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient stochastique sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Stochastic***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad = [ 160. 4000.]\n",
      "i = 0 Theta = [6.984 7.6  ]\n",
      "Grad = [ 108.768 2066.592]\n",
      "i = 1 Theta = [6.9731232 7.3933408]\n",
      "Grad = [  373.734048 28403.787648]\n",
      "i = 2 Theta = [6.9357498  4.55296204]\n",
      "Grad = [ -2.64556342 -37.03788794]\n",
      "i = 3 Theta = [6.93601435 4.55666582]\n",
      "Grad = [ -14.0680168  -379.83645363]\n",
      "i = 4 Theta = [6.93742115 4.59464947]\n",
      "Grad = [ -3.90707574 -66.4202875 ]\n",
      "i = 5 Theta = [6.93781186 4.6012915 ]\n",
      "Grad = [ -17.23655441 -586.04284984]\n",
      "i = 6 Theta = [6.93953552 4.65989578]\n",
      "Grad = [  -45.21781211 -3662.64278064]\n",
      "i = 7 Theta = [6.9440573  5.02616006]\n",
      "Grad = [ 12.39948047 595.17506236]\n",
      "i = 8 Theta = [6.94281735 4.96664255]\n",
      "Grad = [9.8856347 0.       ]\n",
      "i = 9 Theta = [6.94182879 4.96664255]\n",
      "Grad = [ 9.61679801 38.46719204]\n",
      "i = 10 Theta = [6.94086711 4.96279584]\n",
      "Grad = [  6.75658441 283.77654532]\n",
      "i = 11 Theta = [6.94019145 4.93441818]\n",
      "Grad = [  5.55198285 183.21543416]\n",
      "i = 12 Theta = [6.93963625 4.91609664]\n",
      "Grad = [ 8.87243215 53.23459291]\n",
      "i = 13 Theta = [6.93874901 4.91077318]\n",
      "Grad = [ 0.95481586 47.740793  ]\n",
      "i = 14 Theta = [6.93865352 4.9059991 ]\n",
      "Grad = [  -6.8548533  -610.08194348]\n",
      "i = 15 Theta = [6.93933901 4.96700729]\n",
      "Grad = [  7.37123233 280.10682845]\n",
      "i = 16 Theta = [6.93860189 4.93899661]\n",
      "Grad = [ -0.37136563 -31.19471314]\n",
      "i = 17 Theta = [6.93863902 4.94211608]\n",
      "Grad = [  1.54199385 111.02355745]\n",
      "i = 18 Theta = [6.93848482 4.93101373]\n",
      "Grad = [ -0.33299887 -24.6419162 ]\n",
      "i = 19 Theta = [6.93851812 4.93347792]\n",
      "Grad = [  5.88571132 176.57133959]\n",
      "i = 20 Theta = [6.93792955 4.91582078]\n",
      "Grad = [ 1.79465436 86.14340931]\n",
      "i = 21 Theta = [6.93775009 4.90720644]\n",
      "Grad = [  -5.71381741 -479.96066204]\n",
      "i = 22 Theta = [6.93832147 4.95520251]\n",
      "Grad = [  2.70904441 216.72355256]\n",
      "i = 23 Theta = [6.93805056 4.93353015]\n",
      "Grad = [ -1.02495363 -84.04619743]\n",
      "i = 24 Theta = [6.93815306 4.94193477]\n",
      "Grad = [  4.88269666 209.95595617]\n",
      "i = 25 Theta = [6.93766479 4.92093918]\n",
      "Grad = [ 0.54615259 32.22300284]\n",
      "i = 26 Theta = [6.93761017 4.91771688]\n",
      "Grad = [ 8.06499166 88.71490827]\n",
      "i = 27 Theta = [6.93680368 4.90884539]\n",
      "Grad = [  4.95125825 133.68397265]\n",
      "i = 28 Theta = [6.93630855 4.89547699]\n",
      "Grad = [  -9.98675486 -948.74171186]\n",
      "i = 29 Theta = [6.93730723 4.99035116]\n",
      "Grad = [ 9.75882838 58.55297029]\n",
      "i = 30 Theta = [6.93633134 4.98449586]\n",
      "Grad = [  8.60132352 352.6542643 ]\n",
      "i = 31 Theta = [6.93547121 4.94923044]\n",
      "Grad = [  3.67705578 224.30040281]\n",
      "i = 32 Theta = [6.9351035 4.9268004]\n",
      "Grad = [  2.1110491  111.88560253]\n",
      "i = 33 Theta = [6.9348924  4.91561184]\n",
      "Grad = [ 8.01324521 88.14569732]\n",
      "i = 34 Theta = [6.93409108 4.90679727]\n",
      "Grad = [  -1.68895672 -104.71531691]\n",
      "i = 35 Theta = [6.93425997 4.9172688 ]\n",
      "Grad = [  -3.36847225 -269.47777964]\n",
      "i = 36 Theta = [6.93459682 4.94421658]\n",
      "Grad = [ 0.49757854 41.79659715]\n",
      "i = 37 Theta = [6.93454706 4.94003692]\n",
      "Grad = [  2.79345034 164.81356981]\n",
      "i = 38 Theta = [6.93426772 4.92355556]\n",
      "Grad = [  -4.65590815 -442.31127435]\n",
      "i = 39 Theta = [6.93473331 4.96778669]\n",
      "Grad = [  4.00664374 364.60458061]\n",
      "i = 40 Theta = [6.93433264 4.93132623]\n",
      "Grad = [ -0.4324003 -32.4300222]\n",
      "i = 41 Theta = [6.93437588 4.93456923]\n",
      "Grad = [  -1.25447885 -106.63070218]\n",
      "i = 42 Theta = [6.93450133 4.9452323 ]\n",
      "Grad = [  5.7066576  216.85298896]\n",
      "i = 43 Theta = [6.93393066 4.923547  ]\n",
      "Grad = [  5.28068151 158.42044525]\n",
      "i = 44 Theta = [6.9334026  4.90770496]\n",
      "Grad = [  -4.16204112 -316.31512501]\n",
      "i = 45 Theta = [6.9338188  4.93933647]\n",
      "Grad = [  5.86384468 193.5068746 ]\n",
      "i = 46 Theta = [6.93323242 4.91998578]\n",
      "Grad = [  -4.69612257 -427.34715361]\n",
      "i = 47 Theta = [6.93370203 4.9627205 ]\n",
      "Grad = [  6.95960297 271.42451577]\n",
      "i = 48 Theta = [6.93300607 4.93557805]\n",
      "Grad = [  -1.85878325 -169.14927617]\n",
      "i = 49 Theta = [6.93319195 4.95249297]\n",
      "Grad = [  5.68576568 250.17369014]\n",
      "i = 50 Theta = [6.93262337 4.92747561]\n",
      "Grad = [  -2.31885147 -194.78352333]\n",
      "i = 51 Theta = [6.93285525 4.94695396]\n",
      "Grad = [  6.78904009 196.88216252]\n",
      "i = 52 Theta = [6.93217635 4.92726574]\n",
      "Grad = [  3.31826948 149.3221266 ]\n",
      "i = 53 Theta = [6.93184452 4.91233353]\n",
      "Grad = [  -5.03961097 -428.36693214]\n",
      "i = 54 Theta = [6.93234848 4.95517022]\n",
      "Grad = [  5.47137878 268.09756015]\n",
      "i = 55 Theta = [6.93180135 4.92836047]\n",
      "Grad = [  7.71441669 115.71625033]\n",
      "i = 56 Theta = [6.9310299  4.91678884]\n",
      "Grad = [  5.36865725 144.95374577]\n",
      "i = 57 Theta = [6.93049304 4.90229347]\n",
      "Grad = [ 8.68850768 52.13104609]\n",
      "i = 58 Theta = [6.92962419 4.89708036]\n",
      "Grad = [  -6.40205438 -505.76229582]\n",
      "i = 59 Theta = [6.93026439 4.94765659]\n",
      "Grad = [  6.30117704 214.24001924]\n",
      "i = 60 Theta = [6.92963428 4.92623259]\n",
      "Grad = [ 1.30224899 75.53044136]\n",
      "i = 61 Theta = [6.92950405 4.91867955]\n",
      "Grad = [ 1.40168087 72.88740521]\n",
      "i = 62 Theta = [6.92936388 4.91139081]\n",
      "Grad = [  3.30164736 122.1609523 ]\n",
      "i = 63 Theta = [6.92903372 4.89917471]\n",
      "Grad = [  -5.06407547 -374.74158482]\n",
      "i = 64 Theta = [6.92954013 4.93664887]\n",
      "Grad = [  6.69152368 167.288092  ]\n",
      "i = 65 Theta = [6.92887097 4.91992006]\n",
      "Grad = [  -4.07616772 -354.62659189]\n",
      "i = 66 Theta = [6.92927859 4.95538272]\n",
      "Grad = [  4.05831059 263.79018855]\n",
      "i = 67 Theta = [6.92887276 4.9290037 ]\n",
      "Grad = [  3.89405629 163.55036433]\n",
      "i = 68 Theta = [6.92848335 4.91264866]\n",
      "Grad = [  3.21826512 122.2940744 ]\n",
      "i = 69 Theta = [6.92816153 4.90041926]\n",
      "Grad = [  5.6739318  119.15256778]\n",
      "i = 70 Theta = [6.92759413 4.888504  ]\n",
      "Grad = [  -4.63929186 -301.5539706 ]\n",
      "i = 71 Theta = [6.92805806 4.9186594 ]\n",
      "Grad = [ 0.58328728 33.24737513]\n",
      "i = 72 Theta = [6.92799973 4.91533466]\n",
      "Grad = [ 7.99336196 87.92698154]\n",
      "i = 73 Theta = [6.9272004  4.90654196]\n",
      "Grad = [ 0.32168076 16.40571885]\n",
      "i = 74 Theta = [6.92716823 4.90490139]\n",
      "Grad = [  -6.31242748 -536.55633618]\n",
      "i = 75 Theta = [6.92779947 4.95855702]\n",
      "Grad = [  6.2086169  273.17914349]\n",
      "i = 76 Theta = [6.92717861 4.93123911]\n",
      "Grad = [  5.72870369 171.86111076]\n",
      "i = 77 Theta = [6.92660574 4.914053  ]\n",
      "Grad = [ 1.77419318 83.38707929]\n",
      "i = 78 Theta = [6.92642832 4.90571429]\n",
      "Grad = [ 9.28714238 27.86142713]\n",
      "i = 79 Theta = [6.92549961 4.90292815]\n",
      "Grad = [ 7.71541843 84.86960272]\n",
      "i = 80 Theta = [6.92472807 4.89444119]\n",
      "Grad = [  -8.94001279 -795.66113828]\n",
      "i = 81 Theta = [6.92562207 4.9740073 ]\n",
      "Grad = [  8.75955072 183.95056511]\n",
      "i = 82 Theta = [6.92474611 4.95561224]\n",
      "Grad = [  5.58826757 268.23684318]\n",
      "i = 83 Theta = [6.92418728 4.92878856]\n",
      "Grad = [  7.28476269 131.12572838]\n",
      "i = 84 Theta = [6.92345881 4.91567599]\n",
      "Grad = [  7.14854917 114.37678671]\n",
      "i = 85 Theta = [6.92274395 4.90423831]\n",
      "Grad = [  -7.39161677 -665.24550959]\n",
      "i = 86 Theta = [6.92348312 4.97076286]\n",
      "Grad = [  6.68935493 361.225166  ]\n",
      "i = 87 Theta = [6.92281418 4.93464034]\n",
      "Grad = [  5.13973296 185.03038661]\n",
      "i = 88 Theta = [6.92230021 4.9161373 ]\n",
      "Grad = [  4.47738781 143.27640982]\n",
      "i = 89 Theta = [6.92185247 4.90180966]\n",
      "Grad = [  3.16676195 107.66990646]\n",
      "i = 90 Theta = [6.92153579 4.89104267]\n",
      "Grad = [ -1.27057593 -64.79937263]\n",
      "i = 91 Theta = [6.92166285 4.89752261]\n",
      "Grad = [ 9.43341613 18.86683227]\n",
      "i = 92 Theta = [6.92071951 4.89563593]\n",
      "Grad = [  -3.51716253 -225.09840185]\n",
      "i = 93 Theta = [6.92107122 4.91814577]\n",
      "Grad = [ 0.83817667 46.09971663]\n",
      "i = 94 Theta = [6.92098741 4.91353579]\n",
      "Grad = [  -5.72158228 -514.94240475]\n",
      "i = 95 Theta = [6.92155956 4.96503003]\n",
      "Grad = [  3.89822498 331.34912352]\n",
      "i = 96 Theta = [6.92116974 4.93189512]\n",
      "Grad = [  3.3042712  158.60501777]\n",
      "i = 97 Theta = [6.92083931 4.91603462]\n",
      "Grad = [  -2.92105908 -222.0004904 ]\n",
      "i = 98 Theta = [6.92113142 4.93823467]\n",
      "Grad = [  3.9127911  187.81397272]\n",
      "i = 99 Theta = [6.92074014 4.91945327]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.92074014, 4.91945327])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grad_Desc(X,Y,0.0001,100,\"stochastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_stochastique = 504.4236842325979\n"
     ]
    }
   ],
   "source": [
    "prediction_stochastique = np.dot(X1, Theta)\n",
    "print('prediction_stochastique =', prediction_stochastique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la descente de gradient mini-batch, un sous-ensemble du jeu d'entrainement, constitué de $\\ q $ exemples tirés aléatoirement, est utilisé à chaque itération pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$. Le nombre $\\ q $  est un hyperparamètre fixé par l'utilisateur avec $\\ q \\le m$.<br>\n",
    "\n",
    "La formule pour le calcul $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ devient:\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i\\in \\mathscr{E}} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Avec $\\mathscr{E} $ l'ensemble des $\\ q $ exemples tirés aléatoirement (cet ensemble est réactualisé à cahque itération).\n",
    "\n",
    "Ecrivez la fonction ***Grad_Batch*** qui calcule le gradient mini_batch en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Batch(X,Y,Theta,q):\n",
    "    # Votre code ici\n",
    "    return(Grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Batch***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$\n",
    "- $\\ q=10$\n",
    "\n",
    "*Remarque : pensez à adapter la fonction ***Grad_Desc*** en ***Grad_Desc_Batch***  pour tenir compte de l'hyperparamètre $\\ q $ spécifique à l'approche mini-batch. Dans la nouvelle fonction ***Grad_Desc_Batch***, le paramètre ***Grad*** prend par défaut la valeur ***Grad_Batch***.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc_Batch(X,Y,Eta,K,q,Grad=Grad_Batch):\n",
    "    # Votre code ici\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grad_Desc_Batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests sur jeu de donnée plus conséquent\n",
    "Vous allez maintenant tester votre code sur un jeu de données plus conséquent et comparer vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***.<br>\n",
    "L'objectif est d'entrainer un régresseur linéaire qui prédit les dépenses de santé par ménage à partir de la base de données HISP (Health Insurance Subsidy Program). La base HISP contient des informations concernant les dépenses de santé de ménages américains avant et après la mise en plce du programme d'assurance HISP (les données sont fictives). <br>\n",
    "\n",
    "\n",
    "Pour des raisons de cohérence, vous n'utiliserez que les données décrivant la situation des ménages avant la mise en œuvre du programme HISP (rond=Before).<br>\n",
    "De plus, seules les variables continues ci-dessous seront utilisées pour entraîner votre modèle :\n",
    "- poverty_index \n",
    "- hhsize\n",
    "- age_sp\n",
    "- age_hh\n",
    "- educ_hh\n",
    "- educ_sp\n",
    "- hospital_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et nettoyage des données\n",
    "Comme il s'agit simplement de comparer vos résultats avec ceux obtenus avec les regérsseurs ScikitLearn, il n'est pas nécessaire de séparer les données en jeu de test et jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"datafinalexam.csv\") # importation des données (le fichier datafinalexam.csv et le notebook doivent être dans le même dossier)\n",
    "data=data.loc[data[\"round\"]==\"Before\"] # filtre pour garder la situation avant la mise en place du dispositif\n",
    "features=[\"poverty_index\",\"hhsize\",\"age_sp\",\"age_hh\",\"educ_hh\",\"educ_sp\",\"hospital_distance\"] # liste des variables à utiliser\n",
    "label=['health_expenditures'] # label\n",
    "data=data[features+label] # selction des colonnes utiles\n",
    "Train_set_features=data[features] # variables du jeu d'entrainement \n",
    "Train_set_label=data[label] # labeldu jeu d'entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres\n",
    "eta=0.00001\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mustapha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=1e-05, learning_rate='constant')"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor # import de la classe SGDRegressor\n",
    "reg = SGDRegressor(fit_intercept=True,learning_rate=\"constant\",eta0=eta,max_iter=K) # Instancition d'un régresseur\n",
    "reg.fit(Train_set_features,Train_set_label) # entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37200136  0.26348105 -1.34086898  0.02423312  0.12008257  0.25357157\n",
      "  0.26443865  0.00662281]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient du modèle linéaire\n",
    "Theta=np.concatenate((reg.intercept_,reg.coef_),axis=None)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.941030553940154\n"
     ]
    }
   ],
   "source": [
    "# Performance du modèle Scikit-Learn (RMSE sur jeu d'entrainement)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "Pred=reg.predict(Train_set_features) # prédiction faites par le modèle\n",
    "MSE=mean_squared_error(Pred,Train_set_label) # MSE entre prédictions et valeurs réelles\n",
    "RMSE=MSE**(1/2) # RMSE\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - Votre approche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données dans un format compatible avec vos fonctions\n",
    "X=Stand_Trans(Train_set_features.values,Train_set_features.shape[0],Train_set_features.shape[1])\n",
    "Y=Train_set_label.values.reshape(Train_set_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez les dimensions de X et de Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta \n",
    "Theta="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des Theta\n",
    "Theta=Grad_Desc(X,Y,eta,K,Grad_Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=\n",
    "MSE=\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=X.dot(Theta)\n",
    "MSE=mean_squared_error(Pred,Y)\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des résultats\n",
    "Comparer vos résultats à ceux obtenus avec ***SGDRegressor***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
